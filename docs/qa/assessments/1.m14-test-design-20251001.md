# Test Design: Story 1.M14 - MÃ³dulo AutoAvaliaÃ§Ã£o

Date: 2025-10-01
Designer: Quinn (Test Architect)

## Test Strategy Overview

- **Total test scenarios**: 42
- **Unit tests**: 8 (19%)
- **Integration tests**: 12 (29%)
- **E2E tests**: 8 (19%)
- **User testing**: 10 (24%)
- **Security tests**: 4 (9%)
- **Priority distribution**: P0: 18, P1: 16, P2: 8

**Approach**: Shift-left for technical validation, extensive user testing for psychological safety and emotional impact with target age group (4th-6th grade). Cannot rely on adult assumptions.

---

## Test Scenarios by Acceptance Criteria

### AC-1: Self-Assessment Presentation

#### Scenarios

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| 1.M14-UNIT-001 | Unit | P0 | Parse AutoAvaliaÃ§Ã£o module configuration | JSON validation |
| 1.M14-UNIT-002 | Unit | P1 | Validate 4-point scale configuration | Config flexibility |
| 1.M14-UNIT-003 | Unit | P1 | Generate question with concept substitution | Template logic |
| 1.M14-INT-001 | Integration | P0 | Render self-assessment UI with 4 options | Core functionality |
| 1.M14-INT-002 | Integration | P1 | Display encouragement message visibly | Psychological safety |
| 1.M14-INT-003 | Integration | P1 | Apply warm color palette (no red/green) | Design system compliance |
| 1.M14-INT-004 | Integration | P2 | Responsive design for 1024px+ screens | Layout validation |
| 1.M14-E2E-001 | E2E | P0 | Student views self-assessment after practice | User journey |
| 1.M14-USER-001 | User | P0 | 4th graders understand question phrasing | Language validation |
| 1.M14-USER-002 | User | P0 | Students notice "no wrong answers" message | Psychological safety |

**Coverage**: AC-1 fully covered with 10 scenarios

**Risk Mitigations**:
- PSY-001 (psychological safety) â†’ USER-001, USER-002, INT-002
- LANG-001 (language complexity) â†’ USER-001, UNIT-003
- UI-001 (scale complexity) â†’ INT-001, USER-001

---

### AC-2: Response Collection

#### Scenarios

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| 1.M14-UNIT-004 | Unit | P0 | Record self-assessment response | Data collection logic |
| 1.M14-UNIT-005 | Unit | P1 | Generate encouraging feedback message | Response validation |
| 1.M14-INT-005 | Integration | P0 | Save response to student_progress.interactions | Persistence |
| 1.M14-INT-006 | Integration | P0 | Display immediate positive acknowledgment | UX feedback |
| 1.M14-INT-007 | Integration | P1 | Progress to activity completion smoothly | Transition flow |
| 1.M14-E2E-002 | E2E | P0 | Complete self-assessment and proceed | Full journey |
| 1.M14-E2E-003 | E2E | P1 | Select "Preciso praticar mais" - receive encouragement | Vulnerable response path |
| 1.M14-E2E-004 | E2E | P1 | Select "Posso ensinar outros" - receive validation | Confident response path |
| 1.M14-USER-003 | User | P0 | Students feel safe selecting "Preciso praticar mais" | Psychological safety validation |
| 1.M14-USER-004 | User | P1 | Response messages feel encouraging (not fake) | Tone validation |
| 1.M14-SEC-001 | Security | P0 | Student A cannot see Student B's responses | Privacy isolation |

**Coverage**: AC-2 fully covered with 11 scenarios

**Risk Mitigations**:
- PSY-001 (psychological safety) â†’ USER-003, USER-004, E2E-003
- DATA-002 (privacy breach) â†’ SEC-001, INT-005
- EMO-001 (emotional harm) â†’ USER-004, E2E-003, E2E-004

---

### AC-3: Metacognitive Development

#### Scenarios

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| 1.M14-UNIT-006 | Unit | P0 | Calculate calibration discrepancy | Algorithm correctness |
| 1.M14-UNIT-007 | Unit | P1 | Track calibration improvement over time | Trend analysis |
| 1.M14-INT-008 | Integration | P0 | Store calibration data in concept_mastery | Data integration |
| 1.M14-INT-009 | Integration | P1 | Generate adaptive challenge suggestions | Recommendation logic |
| 1.M14-INT-010 | Integration | P1 | Update Jeremias state based on calibration | System integration |
| 1.M14-E2E-005 | E2E | P1 | Calibration improves over 4 weeks | Long-term validation |
| 1.M14-E2E-006 | E2E | P2 | Teacher sees calibration trend dashboard | Teacher UX |
| 1.M14-USER-005 | User | P0 | Students understand "thinking about learning" concept | Metacognition comprehension |
| 1.M14-USER-006 | User | P1 | Teachers interpret calibration data correctly | Training validation |

**Coverage**: AC-3 fully covered with 9 scenarios

**Risk Mitigations**:
- CAL-001 (teacher misuse) â†’ USER-006, E2E-006
- CAL-002 (algorithm inaccuracy) â†’ UNIT-006, UNIT-007, E2E-005

---

## Psychological Safety & Privacy Tests

### Critical User Testing Scenarios

| ID | Level | Priority | Test Scenario | Method |
|---|---|---|---|---|
| 1.M14-USER-007 | User | P0 | Emotional response during self-assessment | Facial expression observation |
| 1.M14-USER-008 | User | P0 | Honesty in responses (compare to actual perf) | Calibration analysis |
| 1.M14-USER-009 | User | P0 | A/B test message tone (encouraging vs neutral) | Quantitative comparison |
| 1.M14-USER-010 | User | P1 | Long-term pattern analysis (safe answer clustering?) | 4-week monitoring |

### Privacy & Security Tests

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| 1.M14-SEC-002 | Security | P0 | RLS policies block cross-student data access | Database security |
| 1.M14-SEC-003 | Security | P0 | Teacher dashboard shows only aggregated data | Privacy design |
| 1.M14-SEC-004 | Security | P1 | Session isolation on shared device | Multi-user scenario |

---

## Risk Coverage Matrix

| Risk ID | Score | Related Test Scenarios | Coverage |
|---|---|---|---|
| PSY-001 | 9 | USER-001, USER-002, USER-003, USER-004, USER-007, USER-009, USER-010 | âœ“ Full |
| DATA-002 | 9 | SEC-001, SEC-002, SEC-003, SEC-004 | âœ“ Full |
| CAL-001 | 8 | USER-006, E2E-006 | âœ“ Full |
| EMO-001 | 6 | USER-004, USER-007, E2E-003, E2E-004 | âœ“ Full |
| TIME-001 | 6 | E2E-002, USER-010 (time tracking) | âœ“ Full |
| LANG-001 | 6 | USER-001, USER-005 (readability) | âœ“ Full |
| CAL-002 | 6 | UNIT-006, UNIT-007, E2E-005 | âœ“ Full |
| DATA-003 | 4 | INT-005 (auto-save), E2E-002 (completion) | âœ“ Partial |
| UI-001 | 4 | INT-001, USER-009 (A/B test scale) | âœ“ Partial |
| PERF-001 | 4 | E2E-006 (dashboard perf) | â—‹ Baseline |
| REL-001 | 4 | INT-003 (emoji rendering) | âœ“ Full |

---

## User Testing Protocol

### Phase 1: Alpha Testing (n=10 students, 4th-6th grade)

**Objective**: Validate psychological safety, language comprehension, emotional impact

**Participants**:
- 4th grade: 3 students (youngest target age)
- 5th grade: 4 students (core demographic)
- 6th grade: 3 students (oldest target age)
- Mix: struggling/average/advanced students (academic diversity)
- Brazilian Portuguese native speakers

**Testing Protocol**:

**Pre-Test** (5 min):
- Explain activity (no judgment, honest feedback valued)
- Obtain parent consent, student assent
- Baseline emotional state (simple emoji self-rating)

**During Test** (15 min):
1. Complete practice activity (BateriaRÃ¡pida - division)
2. AutoAvaliaÃ§Ã£o module appears
3. Observer notes:
   - Facial expressions (confusion? anxiety? ease?)
   - Hesitation time before selecting response
   - Reading behaviors (re-reading questions? skipping?)
   - Body language (leaning in? pulling back?)
4. Student thinks aloud (encouraged but not required)

**Post-Test Interview** (10 min):
1. "How did this activity make you feel?" (open-ended)
2. "Did you understand what the questions were asking?" (yes/no/sort of)
3. "Did you feel comfortable being honest about how you're doing?" (yes/no/sort of)
4. "What did you like or not like about the colors, emojis, or words?"
5. "Would you want to do this again?" (yes/no/maybe)
6. Show them alternative phrasing options: "Which sounds better to you?"

**Data Collection**:
- Video recording (facial expressions - with consent)
- Written observer notes (behavior, comments)
- Response selections (what did they choose?)
- Actual performance comparison (calibration baseline)
- Survey responses (structured + open-ended)

**Analysis**:
- Qualitative: Thematic analysis of interviews
- Quantitative: Response distribution, time spent
- Behavioral: Facial expression coding (positive/neutral/negative)
- Calibration: Self-rating vs actual performance

**Success Criteria**:
- â‰¥80% understand questions clearly
- â‰¥80% feel comfortable being honest
- â‰¥70% positive/neutral emotional response
- Response distribution varied (no clustering at "safe" answer)
- No major design issues (confusion, distress)

---

### Phase 2: A/B Testing (n=40 students, 2 groups)

**Objective**: Optimize message tone for psychological safety

**Groups**:
- **Group A (n=20)**: Current encouraging tone ("EstÃ¡ tudo bem! Praticar mais vai te ajudar.")
- **Group B (n=20)**: Alternative neutral tone ("VocÃª estÃ¡ aprendendo. Continue praticando.")

**Matching**: Age, academic level, gender balanced across groups

**Testing Protocol**:
1. Random assignment to Group A or B
2. Complete same practice activity + self-assessment
3. Post-test survey (same for both groups)
4. Teacher observation notes

**Metrics**:
- Completion rate (did they finish?)
- Response distribution (honest spread vs clustering)
- Emotional response survey (5-point scale: very negative to very positive)
- Time spent (rushing through = discomfort?)
- Calibration accuracy baseline

**Analysis**:
- Chi-square test: Response distribution difference between groups
- T-test: Emotional response score difference
- ANOVA: Completion rate, time spent
- Qualitative: Preference explanations

**Decision Rule**:
- If Group A significantly better (p<0.05): Keep current tone
- If Group B significantly better: Switch to neutral tone
- If no significant difference: Choose based on qualitative feedback

---

### Phase 3: Long-term Monitoring (4 weeks, ongoing users)

**Objective**: Detect psychological safety failures over time

**Sample**: All students using AutoAvaliaÃ§Ã£o (nâ‰ˆ30 initially)

**Data Collection**:
- Weekly self-assessment responses (automated)
- Weekly calibration accuracy (automated)
- Weekly emotional check-in survey (optional, 2 questions)
- Teacher observation reports (bi-weekly)

**Red Flags (Trigger Manual Review)**:
- **Safe answer clustering**: >70% students selecting "3" or "4" consistently
- **Calibration stagnation**: No improvement in accuracy over 4 weeks
- **Completion rate drop**: <80% complete self-assessment
- **Emotional response decline**: Negative survey trend
- **Teacher concerns**: Reports of student discomfort

**Analysis**:
- Time series: Response distribution trends
- Time series: Calibration accuracy trends
- Cohort analysis: Age groups, academic levels
- Teacher feedback: Qualitative themes

**Intervention**:
- If red flags detected: Pause rollout, investigate, iterate design
- If positive trends: Continue monitoring, plan wider rollout

---

## Test Implementation Priority

### Phase 1: P0 Tests (Critical - Block Release)

**Unit Tests** (4-6 hours):
- 1.M14-UNIT-001: Configuration parsing
- 1.M14-UNIT-004: Response recording logic
- 1.M14-UNIT-006: Calibration discrepancy calculation
- 1.M14-UNIT-007: Calibration trend analysis

**Integration Tests** (8-12 hours):
- 1.M14-INT-001: Render self-assessment UI
- 1.M14-INT-002: Display encouragement message
- 1.M14-INT-005: Save response to database
- 1.M14-INT-006: Immediate positive acknowledgment
- 1.M14-INT-008: Store calibration data

**E2E Tests** (6-8 hours):
- 1.M14-E2E-001: Student views after practice
- 1.M14-E2E-002: Complete and proceed

**Security Tests** (4-6 hours):
- 1.M14-SEC-001: Cross-student privacy isolation
- 1.M14-SEC-002: RLS policy verification
- 1.M14-SEC-003: Teacher dashboard aggregation only

**User Tests** (40-50 hours):
- 1.M14-USER-001: 4th graders understand questions (n=10)
- 1.M14-USER-002: Notice "no wrong answers" (n=10)
- 1.M14-USER-003: Feel safe with vulnerable response (n=10)
- 1.M14-USER-007: Emotional response observation (n=10)
- 1.M14-USER-008: Honesty in responses (n=10)

**Total Phase 1**: ~70-90 hours (includes recruitment, testing, analysis)

---

### Phase 2: P1 Tests (Important - Validate Before Scale)

**Unit Tests** (2-4 hours):
- 1.M14-UNIT-002: Scale configuration validation
- 1.M14-UNIT-003: Question template logic
- 1.M14-UNIT-005: Feedback message generation

**Integration Tests** (6-8 hours):
- 1.M14-INT-003: Color palette compliance
- 1.M14-INT-007: Smooth activity completion transition
- 1.M14-INT-009: Adaptive challenge suggestions
- 1.M14-INT-010: Jeremias state update

**E2E Tests** (6-8 hours):
- 1.M14-E2E-003: "Preciso praticar mais" path
- 1.M14-E2E-004: "Posso ensinar outros" path
- 1.M14-E2E-005: Calibration improvement (4 weeks)
- 1.M14-E2E-006: Teacher dashboard UX

**Security Tests** (2-3 hours):
- 1.M14-SEC-004: Session isolation on shared device

**User Tests** (60-80 hours):
- 1.M14-USER-004: Response messages feel encouraging (n=10)
- 1.M14-USER-005: Understand metacognition concept (n=10)
- 1.M14-USER-006: Teachers interpret calibration correctly (n=5 teachers)
- 1.M14-USER-009: A/B test message tone (n=40)
- 1.M14-USER-010: Long-term pattern analysis (4 weeks, ongoing)

**Total Phase 2**: ~76-103 hours

---

### Phase 3: P2 Tests (Nice to Have - Future Optimization)

**Integration Tests** (4-6 hours):
- 1.M14-INT-004: Responsive design validation

**Total Phase 3**: ~4-6 hours

---

## Test Data Requirements

### Valid Configurations

**Config Set 1: Standard 4-Point Scale**
```json
{
  "moduleType": "AutoAvaliaÃ§Ã£o",
  "config": {
    "concepts": ["divisÃ£o"],
    "scaleType": "4-point",
    "questionTemplate": "Como vocÃª se sente sobre {concept} agora?",
    "showOverallQuestion": true,
    "collectConfidence": true,
    "options": [
      {
        "value": 1,
        "label": "Preciso praticar mais",
        "emoji": "ðŸ˜…",
        "color": "#FED7B8",
        "response": "EstÃ¡ tudo bem! Praticar mais vai te ajudar."
      },
      {
        "value": 2,
        "label": "Estou melhorando",
        "emoji": "ðŸ™‚",
        "color": "#B7E4C7",
        "response": "Ã“timo! VocÃª estÃ¡ no caminho certo."
      },
      {
        "value": 3,
        "label": "JÃ¡ domino",
        "emoji": "ðŸ˜Š",
        "color": "#A8DADC",
        "response": "ParabÃ©ns! VocÃª estÃ¡ indo muito bem."
      },
      {
        "value": 4,
        "label": "Posso ensinar outros",
        "emoji": "ðŸŒŸ",
        "color": "#F1FAEE",
        "response": "IncrÃ­vel! Ensinar outros Ã© uma Ã³tima forma de aprender ainda mais."
      }
    ]
  }
}
```

**Config Set 2: Multiple Concepts**
```json
{
  "moduleType": "AutoAvaliaÃ§Ã£o",
  "config": {
    "concepts": ["divisÃ£o", "multiplicaÃ§Ã£o", "interpretaÃ§Ã£o_texto"],
    "scaleType": "4-point",
    // ... same options
  }
}
```

**Config Set 3: Alternative 3-Point Scale**
```json
{
  "moduleType": "AutoAvaliaÃ§Ã£o",
  "config": {
    "concepts": ["adiÃ§Ã£o"],
    "scaleType": "3-point",
    "options": [
      {"value": 1, "label": "Preciso praticar mais", ...},
      {"value": 2, "label": "Estou melhorando", ...},
      {"value": 3, "label": "JÃ¡ domino", ...}
    ]
  }
}
```

### Test Student Data

**Student Profiles for Testing**:

**Profile A: Underconfident (Imposter Syndrome)**
- Self-assessment: 1 or 2 ("Preciso praticar mais")
- Actual performance: 0.85 (85% accuracy)
- Calibration: Underestimating significantly
- Expected recommendation: "Encourage recognition of competence"

**Profile B: Well-Calibrated**
- Self-assessment: 3 ("JÃ¡ domino")
- Actual performance: 0.82 (82% accuracy)
- Calibration: Accurate
- Expected recommendation: "Continue current path"

**Profile C: Overconfident**
- Self-assessment: 4 ("Posso ensinar outros")
- Actual performance: 0.62 (62% accuracy)
- Calibration: Overestimating significantly
- Expected recommendation: "Provide worked examples to build understanding"

**Profile D: Improving Calibration**
- Week 1: Self=2, Actual=0.55 (discrepancy = -0.075)
- Week 2: Self=2, Actual=0.60 (discrepancy = -0.025)
- Week 3: Self=3, Actual=0.78 (discrepancy = -0.095)
- Week 4: Self=3, Actual=0.81 (discrepancy = -0.025)
- Trend: Calibration improving, confidence increasing with performance

---

## Mock/Stub Strategy

### Unit Test Mocks

**Calibration Algorithm**:
- Mock actual performance data (pre-defined values)
- Stub concept_mastery queries
- Fake timestamp for trend analysis

**Configuration Parser**:
- Mock JSON config variations
- Stub validation library

### Integration Test Mocks

**Minimal Mocking** (prefer real components):
- Mock Supabase client (use Supabase local instance instead)
- Stub teacher dashboard aggregation queries (test data)
- Real UI rendering (no mocking React)

### E2E Test Mocks

**No Mocking** (real system):
- Real database (test instance)
- Real UI components
- Real student journey
- Mock only: External analytics services (if applicable)

### User Test Mocks

**No Mocking** (authentic experience):
- Real module in staging environment
- Real student accounts (test data)
- Real practice activities precede self-assessment
- Authentic end-to-end student experience

---

## Test Environment Requirements

### Unit Tests
- **Framework**: Vitest
- **Assertion**: Expect API
- **Mocking**: vi.mock
- **Coverage**: c8 (target 90%+)

### Integration Tests
- **Framework**: Vitest + React Testing Library
- **Rendering**: @testing-library/react
- **User events**: @testing-library/user-event
- **Assertions**: @testing-library/jest-dom
- **Database**: Supabase local instance

### E2E Tests
- **Framework**: Playwright
- **Browsers**: Chromium (primary), Firefox, Safari
- **Viewports**: Desktop 1024x768, 1366x768, 1920x1080
- **Database**: Supabase test instance (isolated)

### User Tests
- **Environment**: Staging server (production-like)
- **Devices**: Desktop computers (school lab simulation)
- **Recording**: Video (facial expressions - with consent)
- **Surveys**: Google Forms or Typeform
- **Analysis**: Qualitative (NVivo or manual) + Quantitative (SPSS/R)

### Security Tests
- **Framework**: Custom scripts + manual testing
- **Tools**: Supabase SQL queries, Postman
- **Access**: Test student accounts A, B (attempt cross-access)

---

## Test Quality Metrics

### Coverage Targets

**Overall**: 85%+ (lower than infrastructure due to user testing focus)

**By Component**:
- Calibration algorithm: 95%+ (critical calculation)
- Configuration parsing: 90%+
- UI components: 80%+ (user testing validates UX)
- Privacy logic: 100% (security critical)

**By Type**:
- Statements: 85%+
- Branches: 80%+
- Functions: 85%+
- Lines: 85%+

### User Testing Targets

**Alpha Testing (n=10)**:
- Comprehension: â‰¥80% understand questions
- Safety: â‰¥80% feel comfortable being honest
- Emotional: â‰¥70% positive/neutral response
- Completion: â‰¥90% finish self-assessment

**A/B Testing (n=40)**:
- Statistical power: 0.80 (detect medium effect size)
- Significance level: Î±=0.05
- Recruitment: â‰¥95% of target sample

**Long-term Monitoring (4 weeks)**:
- Data completeness: â‰¥90% sessions logged
- Calibration trend: Positive slope in â‰¥60% students
- Completion rate: â‰¥85% sustained

### Performance Targets

**Test Execution Time**:
- Unit tests: <5 seconds (all)
- Integration tests: <30 seconds (all)
- E2E tests: <3 minutes (all)
- User tests: ~30 min per participant

### Quality Gates

**Automated checks pass**:
- âœ“ All P0 tests pass (100%)
- âœ“ Coverage â‰¥ 85%
- âœ“ Security tests pass (privacy verified)
- âœ“ Performance targets met (<3min self-assessment)

**User validation complete**:
- âœ“ Alpha testing shows positive response (â‰¥70%)
- âœ“ A/B testing identifies optimal tone
- âœ“ Long-term monitoring shows no red flags (4 weeks minimum)

---

## Recommended Test Execution Order

### Local Development (fast feedback):
1. Unit tests for current file (watch mode)
2. Related integration tests
3. Pre-commit: All unit tests + lint

### CI/CD Pipeline:
1. **Fail Fast** (0-3 min):
   - P0 unit tests
   - Lint + type check
   - Security baseline (RLS syntax)
2. **Core Functionality** (3-10 min):
   - All unit tests
   - P0 integration tests
   - Security tests (privacy isolation)
3. **Full Validation** (10-25 min):
   - All integration tests
   - E2E P0 tests
   - Calibration algorithm edge cases
4. **Comprehensive** (25-40 min):
   - All E2E tests
   - Cross-browser testing
   - Accessibility audit

### User Testing (separate timeline):
- **Alpha**: 2-3 weeks (recruitment + testing + analysis)
- **A/B**: 3-4 weeks (recruitment + testing + statistical analysis)
- **Long-term**: 4 weeks minimum (ongoing monitoring)

### Pre-Release:
- All automated tests (P0, P1, P2)
- User testing complete (alpha + A/B)
- Long-term monitoring initiated (data collection setup)
- Security audit passed
- Accessibility audit passed

---

## Test Maintenance Strategy

### Test Reviews
- Code review includes tests (especially calibration logic)
- User testing protocols reviewed by UX expert + psychologist (if available)
- Security tests reviewed by architect

### Test Data Management
- Student test accounts (anonymized): StudentA_Test, StudentB_Test
- Practice activity data (pre-populated for consistent baselines)
- Teacher test account (view aggregated data)

### Test Documentation
- Each test has clear Given-When-Then comment
- User testing protocols documented (replicable)
- Analysis procedures documented (transparent)

---

## Future Test Enhancements

**After MVP** (Module working in production):
- Longitudinal study: Calibration accuracy improvement over 6 months
- Cross-cultural validation: Test with students in different Brazilian regions
- Accessibility: Expanded testing with students with disabilities
- Multi-language: Validate translations (Portuguese regional variations)

**Performance Monitoring** (Post-launch):
- Real User Monitoring (RUM) for actual time spent
- Completion rate tracking (detect drop-offs)
- Emotional response surveys (quarterly)
- Teacher satisfaction surveys (bi-annual)

---

## Appendix: Test Scenario Details

### Critical Path: Psychological Safety Validation

**1.M14-USER-003: Students feel safe selecting "Preciso praticar mais"**

**Objective**: Verify students do not fear selecting the most vulnerable response option.

**Method**:
- **Participants**: n=10 students (mix of struggling/average/advanced)
- **Setup**: Students complete practice activity where they struggled (50-60% accuracy)
- **Observation**: Do they select "1" or "2" honestly, or jump to "3" (safe answer)?
- **Post-test**: Interview - "Why did you choose that option?"

**Success Criteria**:
- â‰¥60% of students with 50-60% accuracy select "1" or "2" (honest response)
- In interviews, no mention of fear, judgment, or pressure
- Facial expressions neutral/positive (not anxious)

**Failure Criteria**:
- >40% select "3" or "4" despite <60% accuracy (dishonesty)
- Interviews reveal fear of judgment, teacher reaction, peer perception
- Facial expressions show anxiety, hesitation, distress

**Action if failure**: Redesign messaging, emphasize safety more, test alternative phrasing.

---

### Critical Path: Privacy Isolation

**1.M14-SEC-001: Student A cannot see Student B's responses**

**Objective**: Verify RLS policies prevent cross-student data access.

**Method**:
- **Setup**: Create StudentA (ID: uuid-a), StudentB (ID: uuid-b) in test database
- **Action 1**: StudentA completes self-assessment, records response "2" for "divisÃ£o"
- **Action 2**: StudentB logs in, attempts to query StudentA's responses:
  ```sql
  SELECT * FROM student_progress WHERE student_id = 'uuid-a';
  ```
- **Expected**: Query returns 0 rows (RLS blocks access)
- **Action 3**: StudentB attempts UI access (manipulate URL to StudentA's profile)
- **Expected**: UI shows error or redirects to StudentB's own data

**Success Criteria**:
- SQL query returns 0 rows (RLS enforced)
- UI prevents access (error message or redirect)
- No StudentA data visible in StudentB's session

**Failure Criteria**:
- SQL query returns StudentA's data (CRITICAL SECURITY FAILURE)
- UI exposes StudentA's responses (CRITICAL PRIVACY BREACH)

**Action if failure**: BLOCK RELEASE. Fix RLS policies immediately. Re-audit all data access paths.

---

### Critical Path: Calibration Algorithm Accuracy

**1.M14-UNIT-006: Calculate calibration discrepancy**

**Objective**: Verify calibration algorithm produces correct discrepancy values.

**Method**:
- **Given**: Student self-rating = 2 ("Estou melhorando"), expected accuracy = 50-75%, midpoint = 0.625
- **When**: Actual performance = 0.75 (75% accuracy)
- **Then**: Discrepancy = 0.75 - 0.625 = +0.125 (slightly better than expected)

**Test Cases**:

| Self-Rating | Label | Expected Range | Midpoint | Actual Perf | Discrepancy | Interpretation |
|---|---|---|---|---|---|---|
| 1 | Preciso praticar mais | 0-0.50 | 0.25 | 0.20 | -0.05 | Slightly underperforming |
| 1 | Preciso praticar mais | 0-0.50 | 0.25 | 0.40 | +0.15 | Overperforming (underconfident) |
| 2 | Estou melhorando | 0.50-0.75 | 0.625 | 0.75 | +0.125 | Well-calibrated |
| 2 | Estou melhorando | 0.50-0.75 | 0.625 | 0.55 | -0.075 | Slightly overconfident |
| 3 | JÃ¡ domino | 0.75-0.90 | 0.825 | 0.85 | +0.025 | Well-calibrated |
| 3 | JÃ¡ domino | 0.75-0.90 | 0.825 | 0.60 | -0.225 | Significantly overconfident |
| 4 | Posso ensinar outros | 0.90-1.00 | 0.95 | 0.95 | 0.0 | Perfect calibration |
| 4 | Posso ensinar outros | 0.90-1.00 | 0.95 | 0.70 | -0.25 | Very overconfident |

**Assertions**:
```javascript
expect(calculateDiscrepancy(1, 0.20)).toBe(-0.05);
expect(calculateDiscrepancy(2, 0.75)).toBe(+0.125);
expect(calculateDiscrepancy(3, 0.60)).toBe(-0.225);
expect(calculateDiscrepancy(4, 0.95)).toBe(0.0);
```

**Success Criteria**: All test cases pass (exact discrepancy values)

**Failure Criteria**: Any discrepancy calculation incorrect (BLOCKS CALIBRATION FEATURE)

---

## Appendix: User Testing Consent & Ethics

### Parental Consent Requirements

**Brazilian Context (LGPD)**:
- Parent/guardian written consent required for minors (<18)
- Clear explanation of data collection (video, responses, performance)
- Right to withdraw consent at any time
- Data anonymization in research reports

**Consent Form Contents**:
1. Purpose of study (validate educational module design)
2. Procedures (what student will do, time required)
3. Risks (minimal - similar to normal classroom activities)
4. Benefits (contribute to better learning tools)
5. Confidentiality (data anonymized, stored securely)
6. Voluntary participation (no penalty for refusal/withdrawal)
7. Contact information (researcher, ethics committee if applicable)

### Student Assent Process

**Age-appropriate explanation**:
- "We're testing a new learning activity to make it better for students like you."
- "You'll do some math problems and then answer questions about how you feel about math."
- "There are no wrong answers. We want your honest thoughts."
- "You can stop anytime if you want."
- "Your parents said it's okay, but you can also say no."

**Verbal assent recorded** (not just parent consent)

### Data Protection

**During testing**:
- Video recordings stored on encrypted device
- Student names replaced with codes (StudentAlpha-001)
- Test data on secure server (not personal devices)

**After testing**:
- Video deleted after analysis (unless parent consents to archiving)
- Anonymized data retained for research (aggregate reports)
- Individual student data never published

### Ethical Considerations

**Minimize risk**:
- No deception (students know they're testing a module)
- No stress induction (supportive environment)
- Debrief after testing (explain purpose, answer questions)
- Compensate for time (school credit, certificate, small gift if appropriate)

**Protect vulnerable students**:
- Option to skip questions if uncomfortable
- Observer trained to detect distress (pause testing if needed)
- Counselor available if student becomes upset (rare but possible)

---

**QA Contact**: For questions about test design or user testing protocols, contact Quinn (Test Architect).

**Next Steps**:
1. Review test design with PO/PM/UX-Expert
2. Prepare user testing recruitment materials (parent consent forms)
3. Set up test environments (staging server, test database)
4. Implement P0 unit tests (calibration algorithm, configuration parsing)
5. Schedule alpha testing sessions (coordinate with school)
