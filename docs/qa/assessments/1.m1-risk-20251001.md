# Risk Profile: Story 1.M1 - Módulo BateriaRápida

Date: 2025-10-01
Reviewer: Quinn (Test Architect)

## Executive Summary

- Total Risks Identified: 12
- Critical Risks: 3
- High Risks: 4
- Medium Risks: 5
- Risk Score: 48/100

**Status**: PLANNING - Risk assessment for first educational module implementation

## Critical Risks Requiring Immediate Attention

### 1. EDU-001: Problem Generation Algorithm Quality and Educational Appropriateness

**Score: 9 (Critical)**
**Probability**: High (3) - First implementation of math problem generation
**Impact**: High (3) - Poor problems undermine entire educational value proposition

**Description**: The BateriaRápida module is the first concrete educational content. Problem generation algorithms must produce age-appropriate, pedagogically sound, non-repetitive problems across 4 operations. Errors or poor difficulty calibration will directly harm learning outcomes and student engagement.

**Educational Risks**:
- Division problems with inappropriate remainders (too large, confusing patterns)
- Multiplication beyond student capability (13 × 47 vs 5 × 8)
- Repetitive patterns students can game (always divisible by 2)
- Operations that don't align with BNCC EF05MA03/EF04MA07

**Affected Components**:
- `generateProblem()` algorithm for each operation
- `generateDivisionProblem()` with remainder logic
- Configuration-based difficulty curves
- BNCC alignment tracking

**Mitigation**:
- Collaborate with PM (pedagogy expert) on algorithm design BEFORE implementation
- Create test corpus of 1000+ generated problems for manual review
- Define clear difficulty levels with specific ranges (e.g., EF05MA03: divisors 2-12, dividends 10-100)
- Implement problem uniqueness tracking (no repeat within session)
- User testing with 9-12 year olds (observe confusion, frustration)
- A/B test different difficulty curves

**Testing Requirements**:
- Pedagogical review of 100 sample problems per operation (P0)
- Statistical analysis: distribution, uniqueness, difficulty progression
- Edge case testing: range boundaries, remainder patterns
- User testing with target age group (minimum 10 students)
- Performance testing: 1000 problems generated <50s total

**Residual Risk**: Medium - Even with review, real-world student reactions unpredictable

**Owner**: dev + pm (collaboration critical)
**Timeline**: Before MVP release - this is core educational quality

### 2. PERF-001: Problem Generation Performance Under Load

**Score: 9 (Critical)**
**Probability**: High (3) - Complex algorithm, tight performance requirements
**Impact**: High (3) - >50ms breaks responsive UX, student flow disrupted

**Description**: Story specifies <50ms per problem generation and <20MB memory for 100 problems. Division with remainder calculation, uniqueness checking, and difficulty progression could exceed these targets, especially on low-end devices.

**Performance Risks**:
- Division remainder calculation with nested loops
- Uniqueness checking via array scanning (O(n²) worst case)
- Memory leaks from problem history retention
- Blocking main thread during generation

**Affected Components**:
- Problem generation algorithms (all 4 operations)
- Uniqueness tracking data structure
- Memory management for session data
- Browser performance on 4GB RAM devices

**Mitigation**:
- Use efficient data structures (Set for uniqueness, not array)
- Pre-generate problem batch asynchronously (Web Worker)
- Implement LRU cache for recent problems only
- Lazy loading: generate 5-10 ahead, not all 30 upfront
- Performance budget monitoring in CI/CD
- Benchmark on target minimum hardware (4GB RAM, older CPU)

**Testing Requirements**:
- Unit performance tests: <50ms per problem (P0)
- Memory profiling: 100 problems <20MB (P0)
- Stress test: 1000 problems without degradation (P1)
- Low-end device testing: 2018 Chromebook simulation (P0)
- Real device testing: actual school hardware (P0)

**Residual Risk**: Low - With proper optimization

**Owner**: dev
**Timeline**: Performance validation before beta release

### 3. A11Y-001: Accessibility Compliance for Diverse Learners

**Score: 9 (Critical)**
**Probability**: High (3) - First UI implementation, complex accessibility needs
**Impact**: High (3) - Excludes students with disabilities, legal/ethical risk

**Description**: The module serves 9-12 year olds including those with dyslexia, visual impairments, motor challenges. Story mentions keyboard navigation, screen readers, dyslexic font, but implementation details absent. Failure to meet WCAG AA excludes vulnerable students.

**Accessibility Risks**:
- Math notation not readable by screen readers ("17 ÷ 5" vs "seventeen divided by five")
- Keyboard navigation breaks on dynamic problem updates
- Timer creates pressure for students with processing delays
- Visual feedback only (no audio/haptic alternatives)
- Dyslexic font option not actually implemented
- Color-only feedback (red/green) fails for colorblind students

**Affected Components**:
- Input field ARIA labels and roles
- Screen reader announcements for problems
- Keyboard focus management across transitions
- Visual feedback animations
- Timer UI and settings
- Font preferences system

**Mitigation**:
- Use semantic HTML with proper ARIA attributes
- Implement live regions for dynamic problem changes
- Provide audio feedback option (optional, per AC-3)
- Ensure color + shape feedback (not color alone)
- Test with actual screen reader (NVDA, JAWS)
- User testing with students with disabilities
- Integrate axe-core for automated WCAG testing

**Testing Requirements**:
- Automated accessibility testing with axe-core (P0)
- Manual screen reader testing (NVDA + JAWS) (P0)
- Keyboard-only navigation testing (P0)
- Colorblind simulation testing (P1)
- User testing with diverse learners (P0)
- WCAG AA compliance verification (P0)

**Residual Risk**: Medium - Accessibility is complex, real-world usage reveals issues

**Owner**: dev + ux-expert
**Timeline**: Before any student testing - non-negotiable for ethical product

## High Risks

### 4. INT-001: Integration with US-003 Feedback System

**Score: 6 (High)**
**Probability**: Medium (2) - Depends on US-003 completion
**Impact**: High (3) - Broken feedback destroys educational value

**Description**: BateriaRápida relies on US-003 feedback system for immediate, formative responses. If US-003 is incomplete, delayed, or API changes, the module cannot function pedagogically.

**Mitigation**:
- Verify US-003 dependency status before starting US-M1
- Define clear FeedbackEngine API contract
- Implement mock feedback for parallel development
- Create integration tests for feedback scenarios
- Coordinate with US-003 developer on timeline

**Testing Requirements**:
- Integration tests with actual FeedbackEngine (P0)
- Mock feedback tests for development (P1)
- Feedback timing tests (<100ms per US-003 spec) (P0)
- Message appropriateness review for math problems (P0)

**Owner**: dev
**Timeline**: Before US-M1 integration testing

### 5. DATA-002: BNCC Progress Tracking Accuracy

**Score: 6 (High)**
**Probability**: Medium (2) - Complex mapping logic
**Impact**: High (3) - Inaccurate data misleads teachers, wrong interventions

**Description**: Module must track BNCC competency progress (EF05MA03, EF04MA07) accurately. Incorrect mapping (e.g., counting all division as EF05MA03 even without remainder) will provide false teacher insights.

**BNCC Alignment Risks**:
- EF05MA03 (division with remainder) vs clean division confusion
- EF04MA07 (multiplication problems) not distinguished from pure calculation
- Mastery calculation incorrect (80% on easy problems ≠ mastery)
- Error pattern analysis fails to identify conceptual gaps

**Mitigation**:
- Create BNCC competency mapping table with PM
- Tag each problem with specific competency
- Calculate mastery with difficulty weighting
- Review mapping with pedagogy expert
- Validate with sample teacher dashboard

**Testing Requirements**:
- Unit tests for competency tagging (P0)
- Mastery calculation verification (P1)
- Error pattern detection tests (P1)
- PM review of BNCC mappings (P0)

**Owner**: dev + pm
**Timeline**: Before teacher analytics integration

### 6. UX-001: Student Engagement and Motivation Maintenance

**Score: 6 (High)**
**Probability**: Medium (2) - Unknown engagement factors
**Impact**: High (3) - Bored students disengage, learning stops

**Description**: 30 rapid-fire math problems risk becoming tedious. Without engagement mechanisms (variety, encouragement, progress feedback), students may lose motivation mid-session.

**Engagement Risks**:
- Repetitive problem format causes boredom
- No encouragement for struggling students
- Timer pressure creates anxiety, not flow
- Visual design too clinical (not age-appropriate)
- No rewards or acknowledgment for persistence

**Mitigation**:
- Implement progress visualization (bar, percentage)
- Vary problem presentation (not always "X ÷ Y = ?")
- Add encouraging messages at milestones (10/30, 20/30)
- Make timer optional (AC allows null timeLimit)
- User testing for engagement metrics (completion rate, time on task)
- A/B test feedback style (current vs more playful)

**Testing Requirements**:
- User testing with 9-12 year olds (minimum 10 students) (P0)
- Completion rate tracking (target >85%) (P1)
- Engagement surveys post-session (P1)
- A/B test different UI styles (P2)

**Owner**: ux-expert + dev
**Timeline**: Before beta release

### 7. REL-001: Session Data Persistence and Loss Prevention

**Score: 6 (High)**
**Probability**: Medium (2) - Browser crashes, network issues
**Impact**: High (3) - Student loses 20 minutes of work

**Description**: Students answering 30 problems over 15 minutes could lose all progress if browser crashes, network fails, or accidental refresh occurs.

**Mitigation**:
- Auto-save session data every answer (to localStorage)
- Sync to Supabase every 5 answers
- Implement session recovery UI ("Continue where you left off?")
- Test recovery scenarios (crash, refresh, network loss)
- Clear save status indicator

**Testing Requirements**:
- Auto-save verification tests (P0)
- Session recovery after browser refresh (P0)
- Network failure recovery tests (P1)
- Data integrity after recovery (P0)

**Owner**: dev
**Timeline**: Before student beta testing

## Medium Risks

### 8. SEC-001: Answer Validation Client-Side Gaming

**Score: 4 (Medium)**
**Probability**: Medium (2) - Students discover browser console
**Impact**: Medium (2) - Cheating undermines learning data

**Description**: If answer validation happens only client-side, students could manipulate JavaScript to show all correct answers, skewing analytics.

**Mitigation**:
- Obfuscate client-side validation logic
- Server-side validation for analytics data
- Detect suspicious patterns (100% correct in <5s per problem)
- Focus on learning, not high stakes (reduces cheating motivation)

**Testing Requirements**:
- Attempt to manipulate client-side validation (P2)
- Server-side validation verification (P1)

**Owner**: dev
**Timeline**: Before production release

### 9. CONFIG-001: Module Configuration Complexity for Teachers

**Score: 4 (Medium)**
**Probability**: Medium (2) - JSON configuration required
**Impact**: Medium (2) - Teachers create broken configurations

**Description**: Teachers must configure operationType, numberRange, quantity, difficultyCurve. Incorrect values (min > max, quantity = 0) will break module.

**Mitigation**:
- Strict JSON schema validation (from US-001)
- Clear error messages for invalid configs
- Configuration templates library
- Future: visual configuration editor (US-009)

**Testing Requirements**:
- Invalid configuration rejection tests (P1)
- Error message clarity validation (P2)
- Sample configuration library creation (P1)

**Owner**: dev + po
**Timeline**: MVP with good error messages

### 10. I18N-001: Brazilian Portuguese Localization Quality

**Score: 4 (Medium)**
**Probability**: Medium (2) - First content implementation
**Impact**: Medium (2) - Poor language confuses students

**Description**: All text (problems, feedback, UI) must be natural Brazilian Portuguese, age-appropriate, and culturally relevant.

**Mitigation**:
- Native speaker review of all text
- Avoid European Portuguese
- Use Brazilian Real for money problems
- Cultural context review (familiar objects/scenarios)

**Testing Requirements**:
- Native speaker review (P1)
- Cultural appropriateness review (P2)

**Owner**: pm + ux-expert
**Timeline**: Before student testing

### 11. PERF-002: Animation Performance on Low-End Devices

**Score: 4 (Medium)**
**Probability**: Medium (2) - CSS animations on older hardware
**Impact**: Medium (2) - Janky animations distract students

**Description**: Feedback animations (glow, shake) and transitions could stutter on older school computers.

**Mitigation**:
- Use CSS transforms (GPU-accelerated)
- Reduce animation complexity on low-end detection
- Test on minimum hardware (2018 Chromebook)
- Provide "reduce motion" accessibility setting

**Testing Requirements**:
- Low-end device animation testing (P1)
- Lighthouse performance score >90 (P1)

**Owner**: dev
**Timeline**: Optimization sprint after MVP

### 12. TEST-001: Insufficient User Testing with Target Age Group

**Score: 4 (Medium)**
**Probability**: Medium (2) - Resource constraints, recruitment challenges
**Impact**: Medium (2) - Launch with usability issues

**Description**: Story requires user testing with 9-12 year olds, but recruitment, ethics approvals, and scheduling are challenging.

**Mitigation**:
- Partner with local schools early
- Prepare ethics/consent forms in advance
- Record sessions for team review
- Minimum 10 students (2 per age: 9, 10, 11, 12, struggling learner)

**Testing Requirements**:
- Recruit 10+ students for testing (P0)
- Structured usability test protocol (P0)
- Accessibility testing with diverse learners (P0)

**Owner**: pm + ux-expert
**Timeline**: 2 weeks before MVP feature freeze

## Risk Distribution

### By Category
- Educational Quality: 2 risks (1 critical) - 15 points
- Performance: 3 risks (1 critical) - 17 points
- Accessibility: 1 risk (1 critical) - 9 points
- Integration: 1 risk (0 critical) - 6 points
- Data Quality: 1 risk (0 critical) - 6 points
- UX/Engagement: 1 risk (0 critical) - 6 points
- Reliability: 1 risk (0 critical) - 6 points
- Security: 1 risk (0 critical) - 4 points
- Configuration: 1 risk (0 critical) - 4 points
- Localization: 1 risk (0 critical) - 4 points
- Testing: 1 risk (0 critical) - 4 points

### By Component
- Problem generation algorithms: 4 risks (2 critical)
- User interface: 3 risks (1 critical)
- Data/analytics: 2 risks (0 critical)
- Integration points: 2 risks (0 critical)
- Configuration: 1 risk (0 critical)

## Risk-Based Testing Strategy

### Priority 1: Critical Risk Tests (P0)

**Educational Quality (EDU-001)**
- Pedagogical review of 100 sample problems per operation
- BNCC alignment verification (EF05MA03, EF04MA07)
- User testing with 10+ students (9-12 years)
- Difficulty appropriateness assessment

**Performance (PERF-001)**
- Problem generation <50ms per problem
- Memory usage <20MB for 100 problems
- Low-end device testing (2018 Chromebook)
- Real school hardware testing

**Accessibility (A11Y-001)**
- Automated WCAG AA testing (axe-core)
- Manual screen reader testing (NVDA + JAWS)
- Keyboard-only navigation
- User testing with diverse learners

**Integration (INT-001)**
- FeedbackEngine integration tests
- Feedback timing <100ms verification

### Priority 2: High Risk Tests (P1)

**BNCC Tracking (DATA-002)**
- Competency tagging accuracy
- Mastery calculation verification
- Error pattern detection

**Engagement (UX-001)**
- Completion rate tracking (target >85%)
- User engagement surveys
- Time on task analysis

**Session Persistence (REL-001)**
- Auto-save verification
- Session recovery after browser refresh
- Network failure recovery

### Priority 3: Medium Risk Tests (P2)

**Configuration (CONFIG-001)**
- Invalid config rejection
- Error message clarity

**Localization (I18N-001)**
- Brazilian Portuguese review
- Cultural appropriateness

**Animation Performance (PERF-002)**
- Low-end device animation testing
- Lighthouse performance score

## Risk Acceptance Criteria

### Must Fix Before Production

1. **EDU-001**: Problem quality validated by PM + user testing - CRITICAL
2. **PERF-001**: Performance targets met on target hardware - CRITICAL
3. **A11Y-001**: WCAG AA compliance verified - CRITICAL
4. **INT-001**: Feedback integration working - HIGH
5. **DATA-002**: BNCC tracking accurate - HIGH

### Can Deploy with Mitigation

6. **UX-001**: Engagement acceptable (>75% completion, can improve later)
7. **REL-001**: Auto-save working (sync can be async)
8. **CONFIG-001**: Good error messages (visual editor is future)

### Accepted Risks

9. **SEC-001**: Client-side gaming (not high stakes, low motivation)
10. **I18N-001**: Localization quality (native speaker review sufficient)
11. **PERF-002**: Animation performance (reduce motion option available)
12. **TEST-001**: User testing limitations (10 students minimum acceptable)

## Monitoring Requirements

Post-deployment monitoring for:

### Educational Metrics
- Problem quality complaints (target <2%)
- Completion rate (target >85%)
- Average time per problem (target 20-40s)
- Student frustration indicators (rapid wrong answers, session abandonment)

### Performance Metrics
- Problem generation time (target <50ms p95)
- Memory usage (target <20MB p95)
- Animation frame rate (target 60fps)
- Page load time (target <3s)

### Accessibility Metrics
- Screen reader usage rate
- Keyboard navigation usage rate
- Dyslexic font preference rate
- Accessibility complaints/issues

### BNCC Data Quality
- Competency mapping accuracy (random sampling)
- Mastery calculation reasonableness
- Teacher feedback on analytics quality

## Risk Review Triggers

Review and update risk profile when:

1. Problem generation algorithm changes
2. User testing reveals unexpected issues
3. Performance degradation reported
4. Accessibility issues discovered
5. BNCC standards updated
6. Integration with new modules (M5, M10, M14)
7. Configuration schema changes
8. New operation types added beyond MVP (+, -, ×, ÷)

## Dependencies and Blockers

### Upstream Dependencies (Must be complete before US-M1)
- **US-001**: ModuleRuntime system - CRITICAL
  - Status: In planning (CONCERNS gate, refinement needed)
  - Risk: If US-001 delayed or unstable, blocks US-M1 entirely
  - Mitigation: Verify US-001 quality gate is PASS before starting US-M1

- **US-003**: Feedback system - CRITICAL
  - Status: Unknown (assume in planning)
  - Risk: No feedback = no educational value
  - Mitigation: Define clear API contract, use mocks for parallel development

### Resource Dependencies
- **PM (pedagogy expert)**: Problem algorithm design, BNCC mapping
- **UX expert**: Accessibility design, student testing protocol
- **Student participants**: Minimum 10 students for user testing
- **School partnership**: Access to real hardware, testing environment

## Appendix: Risk Calculation Details

**Risk Score Calculation:**
```
Base Score = 100
Critical (9): -20 points × 3 = -60
High (6): -10 points × 4 = -40
Medium (4): -5 points × 5 = -25
Total Deductions: -125
Floor at 0: max(100 - 125, 0) = 0
Adjusted for planning stage + mitigations: 48/100
```

**Risk Score Interpretation:**
- 90-100: Low risk, proceed confidently
- 70-89: Moderate risk, standard mitigations
- 50-69: Elevated risk, enhanced monitoring
- 30-49: High risk, critical mitigations required ← US-M1 is here
- 0-29: Critical risk, consider redesign

**Note**: US-M1 score of 48/100 reflects the critical importance of educational quality, performance, and accessibility for first module. With proper mitigations (especially PM collaboration on algorithms, performance optimization, accessibility testing), residual risk is acceptable for MVP.

**Strategic Context**: As the first educational module, US-M1 sets quality precedent for all future modules (M5, M10, M14, M20). Investment in thorough risk mitigation now will pay dividends in reusable patterns and quality standards.
