# Risk Profile: Story 1.M14 - MÃ³dulo AutoAvaliaÃ§Ã£o

Date: 2025-10-01
Reviewer: Quinn (Test Architect)

## Executive Summary

- Total Risks Identified: 11
- Critical Risks: 3
- High Risks: 4
- Medium Risks: 4
- Risk Score: 42/100

**Status**: PLANNING - Risk assessment for implementation guidance with focus on psychological safety and privacy

## Critical Risks Requiring Immediate Attention

### 1. PSY-001: Psychological Safety Failure - Students Fear Honest Responses

**Score: 9 (Critical)**
**Probability**: High (3) - Children naturally give "safe" answers to avoid judgment
**Impact**: High (3) - Destroys metacognitive development, corrupts calibration data, defeats module purpose

**Description**: The core value of AutoAvaliaÃ§Ã£o depends on students feeling safe to be honest about struggles. If language, tone, or design creates fear of judgment, students will select "safe" answers (overestimating competence) rather than authentic self-assessment. This corrupts calibration tracking and prevents genuine metacognitive development.

**Affected Components**:
- Question phrasing ("Como vocÃª se sente..." vs "Quanto vocÃª sabe...")
- Response option labels (non-judgmental language)
- Color palette (warm, not red/green grades)
- Response feedback messages (encouraging regardless of selection)
- Privacy protection (responses not shared with peers)

**Mitigation**:
- Psychological safety protocol:
  - NEVER use grading colors (red/green)
  - NEVER use comparative language ("better than", "worse than")
  - ALWAYS emphasize "no wrong answers" prominently
  - ALWAYS validate all responses positively
  - Model struggle as normal and valuable
- User testing with actual students (4th-6th grade):
  - Observe selection patterns (clustering at "safe" answers?)
  - Exit interviews: "How did this make you feel?"
  - A/B test message variations
- Privacy enforcement:
  - Student responses NEVER visible to peers
  - Teacher dashboard shows aggregated trends only
  - Individual calibration data private to teacher-student context

**Testing Requirements**:
- PSY-TEST-001: Student user testing (n=10) - measure emotional response
- PSY-TEST-002: A/B test response message tone (encouraging vs neutral)
- PSY-TEST-003: Track selection patterns over 4 weeks (detect "safe answer clustering")
- PSY-TEST-004: Privacy verification - ensure peer isolation
- PSY-TEST-005: Teacher interpretation training (avoid misusing calibration data)

**Residual Risk**: Medium - Human psychology is complex, requires iterative refinement

**Owner**: ux-expert + dev + qa
**Timeline**: Before beta testing with students

---

### 2. DATA-002: Privacy Breach - Student Self-Assessments Exposed to Peers

**Score: 9 (Critical)**
**Probability**: Medium (2) - Implementation error or UI mistake could expose data
**Impact**: Critical (4) - Immediate loss of trust, students stop using honestly, psychological harm

**Description**: If student self-assessment responses become visible to peers (through implementation error, shared screen, or UI design flaw), students will lose trust immediately and stop providing honest responses. This is especially harmful for children who selected "Preciso praticar mais" (need more practice) - exposure could cause embarrassment, bullying, or withdrawal.

**Affected Components**:
- Database schema (student_progress.interactions)
- Dashboard rendering (student vs teacher views)
- State management (ensure no cross-student data leakage)
- UI design (no visible peer responses)

**Mitigation**:
- Technical safeguards:
  - Supabase Row Level Security (RLS) enforced
  - Student view queries filtered by `student_id` only
  - Teacher view shows aggregated data, not individual selections
  - No peer-to-peer data access in any context
- UI design:
  - Clear visual separation: "This is private between you and your teacher"
  - No peer comparison features (even anonymized - Future V2+)
  - No "recent responses" feeds or activity streams
- Testing:
  - Security audit: Attempt cross-student data access
  - UI review: Verify no peer data visible
  - Privacy compliance: LGPD considerations

**Testing Requirements**:
- SEC-TEST-001: Attempt to access other student's self-assessment data
- SEC-TEST-002: Verify RLS policies block cross-student queries
- SEC-TEST-003: UI audit - confirm no peer response visibility
- SEC-TEST-004: Session isolation - multiple students on same device
- SEC-TEST-005: Teacher dashboard - verify aggregation, no individual exposure

**Residual Risk**: Low - With proper RLS and testing

**Owner**: dev + qa + architect
**Timeline**: Must be verified before first student use

---

### 3. CAL-001: Calibration Data Misuse by Teacher

**Score: 8 (Critical)**
**Probability**: High (3) - Teachers may misinterpret data without training
**Impact**: High (3) - Students punished for honest self-assessment, system trust destroyed

**Description**: Teachers may misuse calibration data (comparing self-assessment vs actual performance) to punish students for "overconfidence" or "underconfidence" rather than using it formatively. Example: "You said you understood but only got 60%, so you're lying or not paying attention." This destroys psychological safety and makes students game the system.

**Affected Components**:
- Teacher dashboard calibration visualization
- Calibration recommendations ("Ana is overestimating division skills")
- Training materials for teachers
- System documentation

**Mitigation**:
- Design mitigations:
  - Frame calibration as "learning opportunity" not "accuracy score"
  - Show calibration trends over time (improving vs static)
  - Recommendations focus on instructional response ("Ana may benefit from worked examples")
  - NEVER frame as "student was wrong about themselves"
- Training requirements:
  - Teacher onboarding must cover calibration interpretation
  - Growth mindset framing: "Calibration improves with practice"
  - Example scenarios: "How to respond to miscalibration"
  - Warning: "Do NOT use calibration data punitively"
- Documentation:
  - Clear teacher guidelines in dashboard
  - Ethical use section in teacher manual
  - Red flag language: "This is NOT about catching students in lies"

**Testing Requirements**:
- TRAIN-TEST-001: Teacher comprehension test after training
- TRAIN-TEST-002: Scenario response test (how would you respond to overconfident student?)
- TRAIN-TEST-003: Dashboard interpretation test (what does this calibration data mean?)
- USER-TEST-001: Observe teacher interactions with calibration data
- USER-TEST-002: Teacher interviews - how are you using this data?

**Residual Risk**: Medium - Requires ongoing monitoring and teacher support

**Owner**: pm + po + ux-expert
**Timeline**: Before teacher dashboard rollout

---

## High Risks

### 4. EMO-001: Emotional Harm from Negative Tone or Design

**Score: 6 (High)**
**Probability**: Medium (2) - Tone is subjective, design requires testing
**Impact**: High (3) - Student anxiety, avoidance of module, negative self-concept

**Description**: If question phrasing, emoji choices, colors, or response messages feel judgmental or anxiety-inducing, students may develop negative associations with self-reflection. This is especially harmful for students already struggling academically.

**Mitigation**:
- Tone guidelines:
  - Warm, encouraging language throughout
  - Growth mindset framing ("Estou melhorando" not "Ainda nÃ£o sei")
  - Validating all emotions ("EstÃ¡ tudo bem! Praticar mais vai te ajudar")
  - No pressure language ("try harder", "you should know this")
- Design choices:
  - Warm color palette (oranges, soft greens) not harsh reds/greens
  - Friendly emojis (ðŸ˜…ðŸ™‚ðŸ˜ŠðŸŒŸ) convey growth, not judgment
  - Adequate spacing, no rushed feeling
  - Optional: "Skip for now" button if student feels overwhelmed
- User testing:
  - Exit survey: "How did this activity make you feel?"
  - Observe facial expressions during use
  - Track completion rates (avoidance behavior?)
  - Long-term: Compare self-esteem measures (if ethical to collect)

**Testing Requirements**:
- EMO-TEST-001: Student emotional response survey (n=20)
- EMO-TEST-002: Facial expression analysis during use (with consent)
- EMO-TEST-003: Completion rate tracking (detect avoidance)
- EMO-TEST-004: A/B test emoji/color variations
- EMO-TEST-005: Long-term tracking - does usage correlate with positive attitudes?

**Owner**: ux-expert + qa
**Timeline**: During alpha testing with pilot students

---

### 5. TIME-001: Self-Assessment Takes Too Long, Fatigues Students

**Score: 6 (High)**
**Probability**: Medium (2) - 2-3 minute target may expand in practice
**Impact**: High (3) - Student frustration, rushed responses, incomplete data

**Description**: If self-assessment module takes longer than 2-3 minutes (as specified), students will rush through it, give random responses, or develop negative attitudes toward the activity. This is especially problematic at the end of a practice session when cognitive fatigue is high.

**Mitigation**:
- Design constraints:
  - Limit to 2-3 questions maximum per session
  - Simple response mechanisms (large buttons, single click)
  - No required text input (future version only)
  - Clear progress indicator ("Question 2 of 3")
  - Auto-save after each response (can close early if needed)
- Performance optimization:
  - Response recorded immediately (<100ms)
  - Smooth transitions (<500ms)
  - No loading states between questions
- Monitoring:
  - Track actual time spent per session
  - Alert if average >3 minutes
  - Track incomplete sessions (student closed early)

**Testing Requirements**:
- TIME-TEST-001: Time tracking instrumentation
- TIME-TEST-002: User testing with stopwatch (n=15)
- TIME-TEST-003: Monitor production time data (95th percentile <3min)
- TIME-TEST-004: A/B test question count (2 vs 3 vs 4)
- TIME-TEST-005: Cognitive load assessment (end of session timing)

**Owner**: dev + ux-expert
**Timeline**: During implementation and beta testing

---

### 6. LANG-001: Language Too Complex for 4th-6th Graders

**Score: 6 (High)**
**Probability**: Medium (2) - Language targets 9-12 year olds, needs validation
**Impact**: High (3) - Students don't understand questions, give random answers, exclude younger students

**Description**: If question phrasing or response labels use vocabulary/syntax beyond 4th-6th grade reading level, students won't understand what's being asked. This leads to random responses, excluding younger/struggling readers, and defeating the metacognitive purpose.

**Mitigation**:
- Language requirements:
  - Target 4th grade reading level (Flesch-Kincaid Grade 4-5)
  - Short sentences (<15 words)
  - Common vocabulary (avoid: "metacogniÃ§Ã£o", use: "pensar sobre como vocÃª aprende")
  - Active voice ("Como vocÃª se sente?" not "QuÃ£o confiante vocÃª estÃ¡?")
  - Brazilian Portuguese idioms appropriate for age
- Validation process:
  - Readability analysis (Flesch Reading Ease >70)
  - Review by Brazilian elementary educator
  - Testing with 4th graders specifically (youngest target)
  - Observe comprehension: "What is this question asking?"
- Alternative support:
  - Optional: Audio narration of questions (accessibility + comprehension)
  - Visual supports (emojis convey meaning)
  - Example: Hover tooltips with simpler phrasing (future)

**Testing Requirements**:
- LANG-TEST-001: Flesch-Kincaid readability analysis
- LANG-TEST-002: Brazilian educator review
- LANG-TEST-003: 4th grade comprehension testing (n=10)
- LANG-TEST-004: Question rephrasing based on student feedback
- LANG-TEST-005: Accessibility audit (screen reader support)

**Owner**: ux-expert + pm + qa
**Timeline**: Before content finalization

---

### 7. CAL-002: Calibration Algorithm Inaccuracy

**Score: 6 (High)**
**Probability**: Medium (2) - Comparing self-rating (1-4) vs accuracy (0-1) requires thoughtful mapping
**Impact**: High (3) - Teachers get wrong recommendations, students get mismatched activities

**Description**: The calibration tracking compares self-assessment rating (1-4 scale) with actual performance (accuracy 0-1.0). If the mapping or discrepancy calculation is incorrect, calibration insights will be misleading. Example: Student rates "3 (JÃ¡ domino)" but only achieves 65% accuracy - is this "overestimating" or "well-calibrated for their growth"?

**Mitigation**:
- Mapping strategy:
  - Define clear thresholds:
    - Rating 1 "Preciso praticar mais": <50% expected accuracy
    - Rating 2 "Estou melhorando": 50-75% expected accuracy
    - Rating 3 "JÃ¡ domino": 75-90% expected accuracy
    - Rating 4 "Posso ensinar outros": >90% expected accuracy
  - Discrepancy = (actual accuracy) - (expected accuracy midpoint)
  - Calibration improves when |discrepancy| decreases over time
- Validation approach:
  - Simulate 100 student scenarios with known calibration states
  - Verify algorithm produces expected recommendations
  - A/B test threshold variations
  - Educator review of recommendations ("Does this make sense?")
- Transparency:
  - Show teacher the calculation logic
  - Document assumptions in teacher dashboard
  - Allow manual override of recommendations

**Testing Requirements**:
- CAL-TEST-001: Algorithm unit tests with edge cases
- CAL-TEST-002: Simulation of 100 student calibration scenarios
- CAL-TEST-003: Educator review of recommendations
- CAL-TEST-004: A/B test threshold variations
- CAL-TEST-005: Long-term validation (do calibration predictions match outcomes?)

**Owner**: dev + pm + architect
**Timeline**: During calibration feature implementation

---

## Medium Risks

### 8. DATA-003: Incomplete or Missing Self-Assessment Data

**Score: 4 (Medium)**
**Probability**: Medium (2) - Students may skip, close browser, or lose connection
**Impact**: Medium (2) - Missing data points reduce calibration accuracy

**Description**: If students skip self-assessment (close browser, lose connection, click away), the activity completion data is incomplete. This reduces calibration tracking accuracy and may create gaps in teacher insights.

**Mitigation**:
- Technical:
  - Auto-save responses immediately after selection
  - Persist to localStorage + Supabase
  - Mark activity as "partially complete" if self-assessment skipped
  - Gentle prompts: "One more step before finishing!"
- Design:
  - Make self-assessment feel like natural end of activity (not "extra step")
  - Clear progress: "Almost done! Last step: quick check-in"
  - Optional: Allow skipping but track skip rate
- Monitoring:
  - Track completion rate (target >90%)
  - Alert if student consistently skips (pattern detection)
  - Teacher notification: "Ana hasn't completed self-assessments in 3 activities"

**Testing Requirements**:
- DATA-TEST-001: Network interruption recovery
- DATA-TEST-002: Browser close recovery
- DATA-TEST-003: Skip rate monitoring
- DATA-TEST-004: Partial completion handling
- DATA-TEST-005: Teacher notification accuracy

**Owner**: dev + qa
**Timeline**: During implementation

---

### 9. UI-001: 4-Point Scale Too Complex or Too Simple

**Score: 4 (Medium)**
**Probability**: Medium (2) - 4 options may be overwhelming or insufficient
**Impact**: Medium (2) - Students struggle to choose, data lacks nuance

**Description**: The story specifies a 4-point scale. If this is too many options, students (especially younger) may struggle to choose. If too few, data lacks nuance (e.g., can't distinguish "somewhat confident" from "very confident").

**Mitigation**:
- Configuration flexibility:
  - Allow `scaleType: "3-point", "4-point", "5-point"` in JSON config
  - Default: 4-point (as specified)
  - A/B test with different age groups
- User research:
  - Test with 4th graders: Is 4 options clear?
  - Test with 6th graders: Is 4 options sufficient?
  - Observe decision time (hesitation suggests complexity)
- Alternative:
  - Visual scale (slider) instead of discrete buttons (future V2)
  - Two-step: First binary ("I'm confident" / "I need practice"), then refinement

**Testing Requirements**:
- UI-TEST-001: User testing with 4-point scale (n=15)
- UI-TEST-002: A/B test 3-point vs 4-point vs 5-point
- UI-TEST-003: Decision time tracking (hesitation analysis)
- UI-TEST-004: Age group comparison (4th vs 6th grade preferences)
- UI-TEST-005: Visual alternatives testing (slider, emoji selector)

**Owner**: ux-expert + pm
**Timeline**: During alpha testing

---

### 10. PERF-001: Teacher Dashboard Calibration Visualization Performance

**Score: 4 (Medium)**
**Probability**: Low (1) - Dashboard is teacher-only, not high concurrency
**Impact**: High (3) - Slow dashboard frustrates teachers, reduces adoption

**Description**: If calibration trend calculations or visualizations are slow (e.g., aggregating data for 30 students over 12 weeks), teachers will avoid using the dashboard. This reduces the formative value of calibration data.

**Mitigation**:
- Technical optimization:
  - Pre-aggregate calibration data (database view or materialized view)
  - Cache teacher dashboard queries (refresh every 5 minutes)
  - Lazy load charts (only when tab opened)
  - Progress indicators for slow queries (>2s)
- Performance targets:
  - Dashboard initial load: <3s
  - Calibration chart render: <1s
  - Student detail view: <2s
- Monitoring:
  - Track dashboard query times (P95 <3s)
  - Alert if slow query detected (>5s)

**Testing Requirements**:
- PERF-TEST-001: Load testing with 30 students, 12 weeks data
- PERF-TEST-002: Query optimization review
- PERF-TEST-003: Dashboard performance profiling
- PERF-TEST-004: Caching effectiveness
- PERF-TEST-005: User perception test ("Does this feel fast?")

**Owner**: dev + architect
**Timeline**: Before teacher dashboard launch

---

### 11. REL-001: Emoji Rendering Inconsistency Across Browsers/Devices

**Score: 4 (Medium)**
**Probability**: Medium (2) - Emojis render differently across platforms
**Impact**: Medium (2) - Tone/meaning changes if emoji looks different

**Description**: The story uses emojis (ðŸ˜…ðŸ™‚ðŸ˜ŠðŸŒŸ) for response options. These render differently across operating systems and browsers (Windows vs macOS vs Linux). If an emoji conveys different emotion on different platforms, students may interpret response options differently.

**Mitigation**:
- Technical approach:
  - Use Twemoji (Twitter emoji library) for consistent rendering
  - SVG fallback if emoji not supported
  - Test on Windows, macOS, Android, iOS, Linux
- Alternative:
  - Custom emoji-style SVG icons (full control over appearance)
  - Ensure text labels are primary ("Estou melhorando") with emoji as decorative
- Accessibility:
  - Emoji as `aria-hidden` decoration, not semantic content
  - Screen reader reads text label only

**Testing Requirements**:
- REL-TEST-001: Cross-platform emoji rendering (5+ platforms)
- REL-TEST-002: Twemoji integration testing
- REL-TEST-003: SVG fallback verification
- REL-TEST-004: Accessibility audit (screen reader emoji handling)
- REL-TEST-005: Visual consistency review

**Owner**: dev + qa
**Timeline**: During implementation

---

## Risk Distribution

### By Category
- Psychological Safety: 2 risks (1 critical, 1 high) - 15 points
- Privacy/Security: 1 risk (1 critical) - 9 points
- Calibration Quality: 2 risks (1 critical, 1 high) - 14 points
- Emotional Impact: 1 risk (1 high) - 6 points
- Usability: 2 risks (2 high) - 12 points
- Data Integrity: 1 risk (1 medium) - 4 points
- UI/UX: 1 risk (1 medium) - 4 points
- Performance: 1 risk (1 medium) - 4 points
- Reliability: 1 risk (1 medium) - 4 points

### By Component
- Question/Response Design: 4 risks (2 critical, 2 high)
- Privacy Architecture: 2 risks (1 critical, 1 high)
- Teacher Dashboard: 2 risks (1 critical, 1 medium)
- Calibration Algorithm: 2 risks (1 high, 1 medium)
- UI Implementation: 1 risk (1 medium)

## Risk-Based Testing Strategy

### Priority 1: Critical Risk Tests (P0)

**Psychological Safety (PSY-001)**
- Student user testing (emotional response)
- A/B testing message tone
- Long-term pattern analysis (safe answer clustering)
- Privacy verification

**Privacy Protection (DATA-002)**
- Security audit (cross-student access attempts)
- RLS policy verification
- UI audit (no peer visibility)
- Session isolation testing

**Teacher Data Misuse Prevention (CAL-001)**
- Teacher training comprehension tests
- Scenario response evaluation
- Dashboard interpretation testing
- Teacher behavior observation

### Priority 2: High Risk Tests (P1)

**Emotional Impact (EMO-001)**
- Student emotional response surveys
- Facial expression analysis
- Completion rate tracking
- A/B testing design variations

**Time Efficiency (TIME-001)**
- Time tracking instrumentation
- User testing with stopwatch
- Production monitoring
- Question count optimization

**Language Accessibility (LANG-001)**
- Readability analysis
- Educator review
- 4th grade comprehension testing

**Calibration Accuracy (CAL-002)**
- Algorithm unit tests
- Simulation of 100 scenarios
- Educator recommendation review

### Priority 3: Medium Risk Tests (P2)

**Data Completeness (DATA-003)**
- Network interruption recovery
- Skip rate monitoring

**Scale Complexity (UI-001)**
- A/B testing scale options
- Age group preference comparison

**Dashboard Performance (PERF-001)**
- Load testing
- Query optimization

**Emoji Consistency (REL-001)**
- Cross-platform rendering
- Twemoji integration

## Risk Acceptance Criteria

### Must Fix Before Production

1. **PSY-001**: Psychological safety validated through student testing - CRITICAL
2. **DATA-002**: Privacy protection verified through security audit - CRITICAL
3. **CAL-001**: Teacher training and safeguards in place - CRITICAL
4. **EMO-001**: Emotional impact testing shows positive/neutral response - HIGH
5. **LANG-001**: Language validated at 4th grade reading level - HIGH

### Can Deploy with Mitigation

6. **TIME-001**: Time tracking in place, can optimize post-launch (acceptable if <4min)
7. **CAL-002**: Calibration algorithm tested, can refine thresholds based on data
8. **DATA-003**: Auto-save implemented, monitoring in place
9. **PERF-001**: Dashboard performance acceptable (target <3s, warn at >5s)

### Accepted Risks

10. **UI-001**: 4-point scale validated, can adjust in future versions
11. **REL-001**: Twemoji or SVG fallback ensures consistency

## Monitoring Requirements

Post-deployment monitoring for:

### Psychological Safety Metrics
- Self-assessment completion rate (target >90%)
- Response distribution (detect clustering at "safe" answers)
- Student sentiment surveys (monthly)
- Teacher feedback on student engagement

### Privacy Compliance
- RLS policy violations (should be zero)
- Cross-student access attempts (security alert)
- Student complaints about exposed data (should be zero)

### Calibration Quality
- Calibration accuracy over time (improving trend expected)
- Teacher usage of calibration dashboard (adoption metric)
- Teacher feedback on recommendation quality

### User Experience
- Time spent per self-assessment session (P95 <3min)
- Skip rate (target <10%)
- Student emotional response surveys (quarterly)
- Teacher satisfaction with insights (quarterly survey)

## Risk Review Triggers

Review and update risk profile when:

1. Student user testing reveals new psychological concerns
2. Privacy breach or near-miss incident occurs
3. Teacher misuses calibration data (observed or reported)
4. Completion rates drop below 80%
5. New research on metacognitive development with children published
6. Scale changes (3-point, 5-point) tested in production
7. Multi-language support added (translation nuance risks)

## Appendix: Risk Calculation Details

**Risk Score Calculation:**
```
Base Score = 100
Critical (9): -20 points Ã— 2 = -40
Critical (8): -15 points Ã— 1 = -15
High (6): -10 points Ã— 4 = -40
Medium (4): -5 points Ã— 4 = -20
Total Deductions: -115
Floor at 0: max(100 - 115, 0) = 0
Adjusted for planning stage + mitigation strength: 42/100
```

**Note**: Lower risk score reflects high sensitivity of metacognitive self-assessment with children. Requires exceptional care, extensive user testing, and iterative refinement. Risk score will improve as mitigation strategies are implemented and validated.

---

**Critical Context**: This module handles sensitive psychological dimensions - self-perception, honesty, vulnerability. Unlike technical modules, errors here cause emotional harm and destroy trust. The risk profile reflects this reality. Every design decision must be validated with actual 4th-6th grade students, not assumptions.

**QA Contact**: For questions about this risk assessment, contact Quinn (Test Architect).

**Next Review Trigger**: After student user testing (alpha) OR when psychological safety concerns emerge OR before production deployment.
