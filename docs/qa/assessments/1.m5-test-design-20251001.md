# Test Design: Story 1.M5 - Módulo IdentificaOperação

Date: 2025-10-01
Designer: Quinn (Test Architect)

## Test Strategy Overview

- **Total test scenarios**: 42
- **Unit tests**: 12 (29%)
- **Integration tests**: 14 (33%)
- **E2E tests**: 6 (14%)
- **Content tests**: 7 (17%)
- **Accessibility tests**: 3 (7%)
- **Priority distribution**: P0: 18, P1: 16, P2: 8

**Approach**: Content-first strategy with emphasis on educational quality validation, accessibility compliance, and keyword effectiveness testing. Standard technical testing augmented with pedagogical validation.

**Special Considerations**:
- Brazilian educator review required for content tests
- Pilot testing with target age group (9-12 years) mandatory
- Dual BNCC competency validation (EF04MA07 + EF04LP15)
- Cultural context validation essential

---

## Test Scenarios by Acceptance Criteria

### AC-1: Problem Presentation

#### Scenarios

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| 1.M5-UNIT-001 | Unit | P0 | Problem text renders from JSON config | Core content rendering |
| 1.M5-UNIT-002 | Unit | P0 | Keyword highlighting applies correctly | Visual feedback for learning |
| 1.M5-UNIT-003 | Unit | P1 | Four operation buttons render properly | UI completeness |
| 1.M5-UNIT-004 | Unit | P1 | Problem counter displays correctly | Progress feedback |
| 1.M5-INT-001 | Integration | P0 | Complete problem UI renders (text + buttons + counter) | Full UI integration |
| 1.M5-INT-002 | Integration | P1 | Keyword highlighting styles match design system | Visual consistency |
| 1.M5-INT-003 | Integration | P1 | Responsive layout 1024px+ verified | Cross-device UX |
| 1.M5-E2E-001 | E2E | P1 | Student views problem with all elements visible | Complete user journey |

**Coverage**: AC-1 fully covered with 8 scenarios

**Risk Mitigations**:
- CONTENT-001 (problem quality) → CONTENT test suite
- UI-001 (keyword highlighting distraction) → INT-002, user testing
- ACCESS-001 (accessibility) → ACCESSIBILITY test suite

---

### AC-2: Operation Selection

#### Scenarios

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| 1.M5-UNIT-005 | Unit | P0 | Operation selection handler fires correctly | Core interaction logic |
| 1.M5-UNIT-006 | Unit | P0 | Correct/incorrect validation logic works | Learning feedback accuracy |
| 1.M5-UNIT-007 | Unit | P1 | Response data structure captures all fields | Analytics data quality |
| 1.M5-INT-004 | Integration | P0 | Click operation button triggers feedback | Complete interaction flow |
| 1.M5-INT-005 | Integration | P0 | Automatic progression after feedback | Smooth user flow |
| 1.M5-INT-006 | Integration | P1 | No penalty tracking (learning mode) | Pedagogical approach verified |
| 1.M5-E2E-002 | E2E | P0 | Student selects operation and sees feedback | Critical user journey |
| 1.M5-E2E-003 | E2E | P1 | Student completes 10-problem session | Full session flow |

**Coverage**: AC-2 fully covered with 8 scenarios

**Risk Mitigations**:
- FEEDBACK-001 (feedback quality) → CONTENT-005, user testing
- ANALYTICS-001 (data accuracy) → UNIT-007, INT-014

---

### AC-3: Educational Feedback

#### Scenarios

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| 1.M5-UNIT-008 | Unit | P0 | Feedback generator creates correct feedback | Core educational logic |
| 1.M5-UNIT-009 | Unit | P0 | Keyword explanation included in feedback | Learning reinforcement |
| 1.M5-UNIT-010 | Unit | P1 | Distractor explanations provided | Addressing misconceptions |
| 1.M5-INT-007 | Integration | P0 | Feedback displays below buttons with animation | Clear visual hierarchy |
| 1.M5-INT-008 | Integration | P0 | Correct operation highlighted visually | Visual learning cue |
| 1.M5-INT-009 | Integration | P1 | Encouragement message displays | Positive reinforcement |
| 1.M5-E2E-004 | E2E | P0 | Student reads feedback and understands mistake | Learning effectiveness |
| 1.M5-CONTENT-005 | Content | P0 | Feedback clarity validated with students (n=10) | Pedagogical validation |

**Coverage**: AC-3 fully covered with 8 scenarios

**Risk Mitigations**:
- FEEDBACK-001 (feedback quality) → CONTENT-005, educator review
- KEYWORD-001 (keyword effectiveness) → CONTENT-003

---

## Content Quality Test Suite (Critical)

**Priority**: P0 - Must complete before implementation

| ID | Level | Priority | Test Scenario | Target | Justification |
|---|---|---|---|---|---|
| 1.M5-CONTENT-001 | Content | P0 | All 70 problems pass technical validation | 100% pass | JSON structure, keyword presence |
| 1.M5-CONTENT-002 | Content | P0 | Reading level analysis within target range | Lexile 600-700 | Age-appropriate text |
| 1.M5-CONTENT-003 | Content | P0 | Keyword recognition testing per operation | ≥75% accuracy | Keyword effectiveness |
| 1.M5-CONTENT-004 | Content | P0 | BNCC dual competency mapping validated | 100% mapped | Regulatory compliance |
| 1.M5-CONTENT-005 | Content | P0 | Feedback comprehension testing with students | ≥80% comprehension | Learning effectiveness |
| 1.M5-CONTENT-006 | Content | P1 | Cultural appropriateness review by educator | Sign-off required | Brazilian context |
| 1.M5-CONTENT-007 | Content | P1 | Problem ambiguity detection testing | ≥80% agreement | Content clarity |

**Content Test Process**:

### Phase 1: Technical Validation (Automated)
```javascript
// Test: 1.M5-CONTENT-001
describe('Problem Bank Technical Validation', () => {
  test('All 70 problems have required structure', () => {
    const problems = loadProblemBank();
    expect(problems.length).toBeGreaterThanOrEqual(70);

    problems.forEach(problem => {
      expect(problem).toHaveProperty('id');
      expect(problem).toHaveProperty('text');
      expect(problem).toHaveProperty('correctOperation');
      expect(problem).toHaveProperty('keywords');
      expect(problem).toHaveProperty('explanation');
      expect(problem).toHaveProperty('districtors');
    });
  });

  test('All problems have at least one keyword', () => {
    const problems = loadProblemBank();
    problems.forEach(problem => {
      expect(problem.keywords.length).toBeGreaterThan(0);
    });
  });

  test('Correct operation is valid enum value', () => {
    const validOps = ['addition', 'subtraction', 'multiplication', 'division'];
    const problems = loadProblemBank();

    problems.forEach(problem => {
      expect(validOps).toContain(problem.correctOperation);
    });
  });
});
```

### Phase 2: Reading Level Analysis (Automated)
```javascript
// Test: 1.M5-CONTENT-002
describe('Reading Level Analysis', () => {
  test('All problems within Lexile 600-700 range', () => {
    const problems = loadProblemBank();
    const lexileAnalyzer = new LexileAnalyzer();

    problems.forEach(problem => {
      const lexileScore = lexileAnalyzer.analyze(problem.text);
      expect(lexileScore).toBeGreaterThanOrEqual(600);
      expect(lexileScore).toBeLessThanOrEqual(700);
    });
  });

  test('All problems have simple sentence structure (max 2 clauses)', () => {
    const problems = loadProblemBank();
    const syntaxAnalyzer = new SyntaxAnalyzer();

    problems.forEach(problem => {
      const clauses = syntaxAnalyzer.countClauses(problem.text);
      expect(clauses).toBeLessThanOrEqual(2);
    });
  });
});
```

### Phase 3: Keyword Recognition Testing (Pilot Study)

**Protocol**:
1. **Participants**: 20+ students (age 9-12)
2. **Method**: Show problem text, ask "Which keyword tells you what operation to use?"
3. **Target**: ≥75% correctly identify primary keyword per operation type
4. **Analysis**: Track keyword recognition rates by operation

**Test Scenarios**:
```
Addition Keywords (n=20 students × 5 problems = 100 responses)
- Target: ≥75 responses identify correct keyword (ganhou, recebeu, juntou, etc.)

Subtraction Keywords (n=20 students × 5 problems = 100 responses)
- Target: ≥75 responses identify correct keyword (perdeu, gastou, deu, etc.)

Multiplication Keywords (n=20 students × 5 problems = 100 responses)
- Target: ≥75 responses identify correct keyword (cada, grupos, vezes, etc.)

Division Keywords (n=20 students × 5 problems = 100 responses)
- Target: ≥75 responses identify correct keyword (repartir, dividir, etc.)
```

### Phase 4: BNCC Alignment Validation (Educator Review)

**Checklist per problem**:
- [ ] EF04MA07 (Math problem-solving): Problem requires operation identification ✓
- [ ] EF04LP15 (Text comprehension): Problem requires locating explicit information ✓
- [ ] Operation type appropriate for 4th-6th grade level
- [ ] Text complexity appropriate for 4th grade reading level
- [ ] Pedagogical value: Problem teaches intended concept

**Educator Sign-off**: All 70 problems must receive educator approval

### Phase 5: Feedback Comprehension Testing (Pilot Study)

**Protocol**:
1. **Participants**: 10+ students (age 9-12)
2. **Method**: Student selects wrong operation, reads feedback, explains in own words
3. **Target**: ≥80% can explain why their choice was wrong and why correct choice is right
4. **Analysis**: Identify feedback that is unclear or confusing

**Test Scenarios**:
```
Scenario: Student selects "subtraction" for addition problem
Feedback: "João GANHOU maçãs, então somamos as que ele tinha com as que ganhou."
Question: "Can you explain why subtraction was wrong and why addition is right?"
Success: Student mentions "ganhou" means adding/getting more
```

### Phase 6: Cultural Appropriateness Review (Educator)

**Checklist**:
- [ ] All names are Brazilian (Ana, João, Pedro, Maria, Carlos, Lucas)
- [ ] All currency references use Real (R$)
- [ ] All contexts are familiar to Brazilian students (school, futebol, merenda)
- [ ] No North American or European cultural references
- [ ] Regional vocabulary appropriate across Brazil (no excessive regionalisms)

### Phase 7: Problem Ambiguity Detection (Pilot Study)

**Protocol**:
1. **Participants**: 20+ students (age 9-12)
2. **Method**: Student reads problem, selects operation without keyword hints
3. **Target**: ≥80% agreement on correct operation (if <80%, problem is ambiguous)
4. **Analysis**: Identify and revise ambiguous problems

**Test Scenarios**:
```
Problem: "João tinha 5 maçãs e ganhou 3. Quantas maçãs ele tem agora?"
Expected: 16+ students select "addition" (80% agreement)
If only 12 students select "addition", problem needs revision
```

---

## Accessibility Test Suite (WCAG 2.1 AA)

**Priority**: P0 - Must pass before MVP launch

| ID | Level | Priority | Test Scenario | Standard | Justification |
|---|---|---|---|---|---|
| 1.M5-ACCESS-001 | Accessibility | P0 | Screen reader announces problem text | WCAG 1.1.1 | Text alternatives |
| 1.M5-ACCESS-002 | Accessibility | P0 | Keyboard navigation for all buttons | WCAG 2.1.1 | Keyboard accessible |
| 1.M5-ACCESS-003 | Accessibility | P0 | Color contrast meets AA ratio (4.5:1) | WCAG 1.4.3 | Visual accessibility |
| 1.M5-INT-010 | Integration | P1 | Text-to-speech reads problem aloud | WCAG 1.2.1 | Audio alternative |
| 1.M5-INT-011 | Integration | P1 | Focus indicators visible on all buttons | WCAG 2.4.7 | Keyboard navigation |
| 1.M5-E2E-005 | E2E | P1 | Complete flow using keyboard only | WCAG 2.1.1 | Keyboard-only users |

**Accessibility Test Protocol**:

### Test 1: Screen Reader Compatibility (NVDA, JAWS)

**Test Steps**:
1. Start NVDA or JAWS
2. Navigate to IdentificaOperação module
3. Verify screen reader announces:
   - Problem text (full text read clearly)
   - Operation button labels ("Adição", "Subtração", "Multiplicação", "Divisão")
   - Feedback text (announced as alert when displayed)
   - Problem counter ("Problema 3 de 10")
   - Correct/incorrect status

**Pass Criteria**:
- All content accessible via screen reader
- No skipped content
- Semantic HTML used (not just CSS styling)
- ARIA labels present where needed

### Test 2: Keyboard Navigation

**Test Steps**:
1. Tab through all interactive elements (operation buttons)
2. Press Enter to select operation
3. Tab through feedback (if focusable)
4. Verify focus order is logical (top to bottom, left to right)

**Pass Criteria**:
- All buttons reachable via Tab
- Focus order follows visual layout
- Focus indicators visible (outline, highlight)
- Enter/Space activates buttons

### Test 3: Color Contrast Analysis

**Test Steps**:
1. Use axe DevTools to analyze color contrast
2. Check contrast ratios for:
   - Problem text (black on white): ≥4.5:1
   - Button text: ≥4.5:1
   - Keyword highlighting: ≥4.5:1 (if using color)
   - Feedback text: ≥4.5:1

**Pass Criteria**:
- All text meets WCAG AA contrast ratio (4.5:1)
- No reliance on color alone for feedback (use icon + color)

### Test 4: Text-to-Speech Integration

**Test Steps**:
1. Click "Listen" button next to problem text
2. Verify Web Speech API reads problem aloud
3. Test with multiple problems
4. Verify playback controls (pause, stop)

**Pass Criteria**:
- Audio clearly narrates problem text
- Portuguese language pronunciation correct
- Playback speed appropriate for comprehension
- Controls accessible via keyboard

---

## Performance & Non-Functional Tests

| ID | Level | Priority | Test Scenario | Target | Justification |
|---|---|---|---|---|---|
| 1.M5-UNIT-011 | Unit | P0 | Problem bank JSON parsing | <100ms | Fast initialization |
| 1.M5-UNIT-012 | Unit | P1 | Keyword highlighting performance | <100ms | No UI jank |
| 1.M5-INT-012 | Integration | P0 | Module initialization time | <2s | US-001 baseline |
| 1.M5-INT-013 | Integration | P1 | Problem rendering performance | <500ms per problem | Smooth transitions |
| 1.M5-INT-014 | Integration | P1 | Analytics data collection accuracy | 100% | Data quality |
| 1.M5-E2E-006 | E2E | P2 | 10-problem session on 3G network | <30s total | Network resilience |

**Performance Test Details**:

### Test: Module Initialization Performance
```javascript
// Test: 1.M5-INT-012
describe('Module Initialization Performance', () => {
  test('Module initializes in under 2 seconds', async () => {
    const startTime = performance.now();

    await render(
      <ModuleRuntime config={identifyOperationConfig} />
    );

    const endTime = performance.now();
    const initTime = endTime - startTime;

    expect(initTime).toBeLessThan(2000); // US-001 baseline
  });
});
```

### Test: Problem Rendering Performance
```javascript
// Test: 1.M5-INT-013
describe('Problem Rendering Performance', () => {
  test('Each problem renders in under 500ms', async () => {
    const problems = loadProblemBank().slice(0, 10);

    for (const problem of problems) {
      const startTime = performance.now();

      await render(<ProblemDisplay problem={problem} />);

      const endTime = performance.now();
      const renderTime = endTime - startTime;

      expect(renderTime).toBeLessThan(500);
    }
  });
});
```

### Test: Analytics Data Accuracy
```javascript
// Test: 1.M5-INT-014
describe('Analytics Data Collection', () => {
  test('Session data captures all required fields', () => {
    const sessionData = collectSessionAnalytics();

    expect(sessionData).toHaveProperty('moduleType', 'IdentificaOperação');
    expect(sessionData).toHaveProperty('sessionData.totalProblems');
    expect(sessionData).toHaveProperty('sessionData.correctSelections');
    expect(sessionData).toHaveProperty('sessionData.errorsByOperation');
    expect(sessionData).toHaveProperty('sessionData.keywordRecognition');
    expect(sessionData).toHaveProperty('sessionData.timeSpent');
    expect(sessionData).toHaveProperty('sessionData.conceptsStruggling');
  });

  test('Keyword recognition rates calculated correctly', () => {
    const sessionData = collectSessionAnalytics();
    const keywordRecognition = sessionData.sessionData.keywordRecognition;

    // Verify each keyword has a recognition rate (0-100%)
    Object.values(keywordRecognition).forEach(rate => {
      expect(rate).toBeGreaterThanOrEqual(0);
      expect(rate).toBeLessThanOrEqual(100);
    });
  });
});
```

---

## Risk Coverage Matrix

| Risk ID | Score | Related Test Scenarios | Coverage |
|---|---|---|---|
| CONTENT-001 | 9 | CONTENT-001 through CONTENT-007 | ✓ Full |
| BNCC-001 | 9 | CONTENT-004, educator review | ✓ Full |
| KEYWORD-001 | 9 | CONTENT-003, pilot testing | ✓ Full |
| CONTENT-002 | 6 | CONTENT-006, cultural review | ✓ Full |
| ACCESS-001 | 6 | ACCESS-001/002/003, INT-010/011 | ✓ Full |
| DATA-003 | 6 | Version testing (future) | ○ Baseline |
| FEEDBACK-001 | 6 | CONTENT-005, educator review | ✓ Full |
| UI-001 | 4 | INT-002, user testing | ✓ Full |
| PERF-003 | 4 | INT-012, E2E-006 | ✓ Full |
| ANALYTICS-001 | 4 | INT-014 | ✓ Full |

**Risk Coverage**: 100% (all 10 risks have explicit test scenarios)

---

## Test Implementation Priority

### Phase 1: Content Validation (Weeks 1-3) - CRITICAL PATH

**Priority**: P0 - BLOCKING for implementation

**Content Creation & Technical Validation** (Week 1):
- Create 70+ problems using template
- Run CONTENT-001 (technical validation)
- Run CONTENT-002 (reading level analysis)
- Fix any issues before educator review

**Brazilian Educator Review** (Week 2):
- Educator reviews all 70 problems
- Run CONTENT-004 (BNCC alignment validation)
- Run CONTENT-006 (cultural appropriateness)
- Iterate based on educator feedback

**Pilot Testing** (Week 3):
- Recruit 20+ students (age 9-12)
- Run CONTENT-003 (keyword recognition testing)
- Run CONTENT-005 (feedback comprehension testing)
- Run CONTENT-007 (ambiguity detection testing)
- Iterate based on pilot results

**Total Phase 1**: ~80-100 hours (content creation + review + pilot testing)

---

### Phase 2: P0 Technical Tests (Weeks 4-5)

**Unit Tests** (8-12 hours):
- 1.M5-UNIT-001: Problem text rendering
- 1.M5-UNIT-002: Keyword highlighting
- 1.M5-UNIT-005: Operation selection handler
- 1.M5-UNIT-006: Correct/incorrect validation
- 1.M5-UNIT-008: Feedback generator
- 1.M5-UNIT-009: Keyword explanation in feedback
- 1.M5-UNIT-011: Problem bank parsing performance

**Integration Tests** (12-16 hours):
- 1.M5-INT-001: Complete problem UI render
- 1.M5-INT-004: Operation selection triggers feedback
- 1.M5-INT-005: Automatic progression
- 1.M5-INT-007: Feedback display with animation
- 1.M5-INT-008: Correct operation highlighted
- 1.M5-INT-012: Module initialization performance

**E2E Tests** (6-8 hours):
- 1.M5-E2E-002: Student selects operation and sees feedback
- 1.M5-E2E-004: Student reads and understands feedback

**Accessibility Tests** (6-8 hours):
- 1.M5-ACCESS-001: Screen reader testing
- 1.M5-ACCESS-002: Keyboard navigation
- 1.M5-ACCESS-003: Color contrast analysis

**Total Phase 2**: ~32-44 hours

---

### Phase 3: P1 Tests (Week 6)

**Unit Tests** (4-6 hours):
- 1.M5-UNIT-003: Four operation buttons render
- 1.M5-UNIT-004: Problem counter display
- 1.M5-UNIT-007: Response data structure
- 1.M5-UNIT-010: Distractor explanations
- 1.M5-UNIT-012: Keyword highlighting performance

**Integration Tests** (6-8 hours):
- 1.M5-INT-002: Keyword highlighting styles
- 1.M5-INT-003: Responsive layout
- 1.M5-INT-006: No penalty tracking
- 1.M5-INT-009: Encouragement message
- 1.M5-INT-010: Text-to-speech integration
- 1.M5-INT-011: Focus indicators
- 1.M5-INT-013: Problem rendering performance
- 1.M5-INT-014: Analytics data accuracy

**E2E Tests** (4-6 hours):
- 1.M5-E2E-001: Student views complete problem UI
- 1.M5-E2E-003: Student completes 10-problem session
- 1.M5-E2E-005: Complete flow using keyboard only

**Total Phase 3**: ~14-20 hours

---

### Phase 4: P2 Tests (Week 7) - Optional for MVP

**E2E Tests** (2-3 hours):
- 1.M5-E2E-006: 10-problem session on 3G network

**User Testing** (8-10 hours):
- Keyword highlighting effectiveness (A/B testing)
- Cognitive load assessment
- Feedback timing validation

**Total Phase 4**: ~10-13 hours

---

## Test Data Requirements

### Valid Problem Bank (70+ Problems)

**Addition Problems (20)** - Sample:
```json
{
  "id": "add_001",
  "text": "João tinha 5 maçãs e ganhou 3. Quantas maçãs ele tem agora?",
  "correctOperation": "addition",
  "keywords": ["ganhou", "agora", "total"],
  "explanation": "João GANHOU maçãs, então somamos as que ele tinha com as que ganhou.",
  "districtors": {
    "subtraction": "João não perdeu maçãs, ele ganhou",
    "multiplication": "Não estamos fazendo grupos iguais",
    "division": "Não estamos dividindo ou repartindo"
  }
}
```

**Subtraction Problems (20)** - Sample:
```json
{
  "id": "sub_001",
  "text": "Pedro tinha 12 figurinhas e deu 5 para Ana. Quantas figurinhas sobraram?",
  "correctOperation": "subtraction",
  "keywords": ["deu", "sobraram"],
  "explanation": "Pedro DEU figurinhas, então subtraímos o que ele deu do que ele tinha.",
  "districtors": {
    "addition": "Pedro não ganhou figurinhas, ele deu",
    "multiplication": "Não estamos fazendo grupos iguais",
    "division": "Não estamos dividindo igualmente entre pessoas"
  }
}
```

**Multiplication Problems (15)** - Sample:
```json
{
  "id": "mult_001",
  "text": "Ana comprou 3 pacotes de balas. Cada pacote tem 8 balas. Quantas balas ela tem no total?",
  "correctOperation": "multiplication",
  "keywords": ["cada", "pacotes", "total"],
  "explanation": "Ana comprou 3 pacotes e CADA pacote tem 8 balas, então multiplicamos 3 × 8.",
  "districtors": {
    "addition": "Não estamos somando o número de pacotes e balas",
    "subtraction": "Ana não perdeu balas",
    "division": "Não estamos dividindo balas entre pessoas"
  }
}
```

**Division Problems (15)** - Sample:
```json
{
  "id": "div_001",
  "text": "Carlos vai repartir 20 chocolates igualmente entre 4 amigos. Quantos chocolates cada amigo vai receber?",
  "correctOperation": "division",
  "keywords": ["repartir", "igualmente", "cada"],
  "explanation": "Carlos vai REPARTIR 20 chocolates IGUALMENTE entre 4 amigos, então dividimos 20 ÷ 4.",
  "districtors": {
    "addition": "Não estamos somando chocolates",
    "subtraction": "Não estamos tirando chocolates",
    "multiplication": "Não estamos fazendo grupos de chocolates, estamos dividindo"
  }
}
```

### Invalid Problem Data (Error Testing)

**Invalid 1: Missing required field**
```json
{
  "id": "invalid_001",
  "text": "João tinha 5 maçãs.",
  // Missing: correctOperation, keywords, explanation, districtors
}
```

**Invalid 2: Wrong operation type**
```json
{
  "id": "invalid_002",
  "text": "João tinha 5 maçãs e ganhou 3.",
  "correctOperation": "exponentiation", // Invalid operation type
  "keywords": ["ganhou"],
  "explanation": "...",
  "districtors": {}
}
```

**Invalid 3: No keywords**
```json
{
  "id": "invalid_003",
  "text": "João tinha 5 maçãs e ganhou 3.",
  "correctOperation": "addition",
  "keywords": [], // Empty keywords array
  "explanation": "...",
  "districtors": {}
}
```

---

## Mock/Stub Strategy

### Unit Test Mocks

**Problem Bank Loading**:
- Mock problem bank JSON (10 sample problems)
- Stub fetch call to return mock data
- Fake problem bank for each operation type

**Analytics Collection**:
- Mock analytics service
- Stub session data collector
- Capture analytics calls for validation

### Integration Test Mocks

**Minimal mocking** (real components preferred):
- Mock external analytics service only
- Use real problem bank (test data)
- Real module runtime (US-001)

### Content Test Mocks

**No mocking** (real content):
- Real problem bank (all 70 problems)
- Real students (pilot testing)
- Real Brazilian educator (review)

### E2E Test Mocks

**No mocking** (real system):
- Real module runtime
- Real problem bank
- Real UI components
- Mock only: Backend APIs (if applicable)

---

## Test Environment Requirements

### Unit Tests
- **Framework**: Vitest
- **Assertion**: Expect API
- **Mocking**: vi.mock
- **Coverage**: c8 (target: 90%+)

### Integration Tests
- **Framework**: Vitest + React Testing Library
- **Rendering**: @testing-library/react
- **User events**: @testing-library/user-event
- **Assertions**: @testing-library/jest-dom

### Content Tests
- **Reading level**: textstat library (Flesch-Kincaid, Lexile approximation)
- **Keyword analysis**: Custom keyword detector
- **Pilot testing**: In-person sessions with students (age 9-12)
- **Educator review**: Google Forms checklist + sign-off document

### Accessibility Tests
- **Screen readers**: NVDA (Windows), JAWS (Windows), VoiceOver (Mac)
- **Automated audit**: axe-core, Pa11y
- **Manual testing**: Keyboard navigation, focus indicators
- **Color contrast**: Chrome DevTools, axe DevTools

### E2E Tests
- **Framework**: Playwright
- **Browsers**: Chromium, Firefox
- **Viewports**: Desktop (1920x1080), Tablet (1024x768)
- **Network**: Fast 3G, Slow 3G

### Performance Tests
- **Profiling**: React DevTools Profiler
- **Memory**: Chrome DevTools Memory
- **Timing**: Performance API (performance.now())

---

## Continuous Integration Strategy

### Pre-commit Hooks
- ESLint + Prettier
- Type checking (TypeScript)
- Unit tests for changed files

### PR Pipeline
1. **Fast checks** (~2 min):
   - Lint
   - Type check
   - Unit tests (all)
2. **Integration tests** (~5 min):
   - Component integration
   - Accessibility automated audit (axe-core)
3. **Content validation** (~3 min):
   - Technical validation (CONTENT-001)
   - Reading level analysis (CONTENT-002)
4. **Build verification** (~2 min):
   - Production build
   - Bundle size check

### Main Branch Pipeline
- All PR checks +
- E2E tests (~10 min)
- Performance benchmarks (~5 min)
- Coverage report (Codecov)

### Content Validation Pipeline (Separate)
- Brazilian educator review (manual, async)
- Pilot testing (scheduled sessions)
- BNCC alignment validation (manual)

---

## Test Quality Metrics

### Coverage Targets

**Overall**: 90%+ (per DoD)

**By Component**:
- Problem display: 95%+ (core UI)
- Operation selection logic: 100% (critical path)
- Feedback generator: 95%+ (educational logic)
- Keyword highlighter: 90%+

**By Type**:
- Statements: 90%+
- Branches: 85%+
- Functions: 90%+
- Lines: 90%+

### Content Quality Targets

**Must achieve before MVP launch**:
- Keyword recognition: ≥75% per operation type
- Feedback comprehension: ≥80%
- Problem ambiguity: ≥80% agreement
- Reading level: 100% within Lexile 600-700
- BNCC alignment: 100% mapped and validated
- Cultural appropriateness: Educator sign-off required

### Performance Targets

**Test execution time**:
- Unit tests: <5 seconds (all)
- Integration tests: <30 seconds (all)
- E2E tests: <3 minutes (all)
- Content tests: ~20 hours (pilot testing)

---

## Recommended Test Execution Order

**Local Development** (fast feedback loop):
1. Unit tests for current file (watch mode)
2. Related integration tests
3. Pre-commit: All unit tests

**CI/CD Pipeline** (optimized for failure detection):
1. **Fail Fast** (0-3 min):
   - P0 unit tests
   - Lint + type check
2. **Core Functionality** (3-10 min):
   - All unit tests
   - P0 integration tests
   - Content technical validation (CONTENT-001/002)
3. **Full Validation** (10-30 min):
   - All integration tests
   - E2E P0 tests
   - Accessibility automated tests
   - Performance tests

**Content Validation** (separate timeline):
1. **Week 1**: Content creation + technical validation
2. **Week 2**: Educator review + BNCC validation
3. **Week 3**: Pilot testing (keyword, feedback, ambiguity)

**Pre-Release** (full suite):
- All technical tests (P0, P1, P2)
- All content validation complete
- Educator sign-off obtained
- Pilot testing targets achieved
- Accessibility audit passed

---

## Test Maintenance Strategy

### Test Reviews
- Code review includes tests
- Content tests reviewed by Brazilian educator
- Pilot testing results documented and archived

### Test Refactoring
- Extract common problem fixtures to shared module
- Create custom matchers for problem validation
- Page Object Model for E2E tests

### Test Documentation
- Each test has clear Given-When-Then comment
- Content test protocols documented
- Pilot testing results archived

---

## Key Success Factors

**Technical Testing**: Standard approach, well-covered by existing patterns

**Content Validation**: CRITICAL PATH - Cannot ship without:
1. Brazilian educator partnership (10-15 hours)
2. Pilot testing with 20+ students (age 9-12)
3. All content quality targets achieved

**Timeline**: 7-9 weeks (content validation is the long pole)

**Budget**: Allocate for Brazilian educator compensation (~10-15 hours professional services)

---

## Future Test Enhancements (Post-MVP)

**After MVP** (Epic 1 complete):
- Visual regression testing (keyword highlighting consistency)
- A/B testing infrastructure (keyword highlighting on/off)
- Learning curve analysis (session 1 vs session 3 improvement)
- Real User Monitoring (RUM) for keyword recognition patterns

**Performance Monitoring** (Post-launch):
- Problem completion rates (identify confusing problems)
- Keyword recognition improvement over time
- Feedback clarity metrics
- Time spent per problem (outliers indicate confusion)

---

## Appendix: Test Scenario IDs

### Naming Convention
```
{epic}.{story}-{LEVEL}-{SEQ}
Example: 1.M5-UNIT-001
```

**LEVEL**:
- UNIT: Unit test
- INT: Integration test
- E2E: End-to-end test
- CONTENT: Content validation test
- ACCESS: Accessibility test

**SEQ**: Sequential number starting from 001

### Full Test ID Index

**Unit (12)**: 1.M5-UNIT-001 through 1.M5-UNIT-012
**Integration (14)**: 1.M5-INT-001 through 1.M5-INT-014
**E2E (6)**: 1.M5-E2E-001 through 1.M5-E2E-006
**Content (7)**: 1.M5-CONTENT-001 through 1.M5-CONTENT-007
**Accessibility (3)**: 1.M5-ACCESS-001 through 1.M5-ACCESS-003

**Total**: 42 test scenarios (35 technical + 7 content validation)

---

**QA Contact**: For questions about this test design, contact Quinn (Test Architect).

**Critical Path**: Content validation (Weeks 1-3) must complete before technical implementation begins.
