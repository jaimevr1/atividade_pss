# Risk Profile: Story 1.002 - Sistema de Atividades Compostas

Date: 2025-10-01
Reviewer: Quinn (Test Architect)

## Executive Summary

- Total Risks Identified: 8
- Critical Risks: 2
- High Risks: 3
- Medium Risks: 3
- Risk Score: 52/100

**Status**: PLANNING - Risk assessment for implementation guidance

## Critical Risks Requiring Immediate Attention

### 1. DATA-002: Complex State Management Across Multiple Modules

**Score: 9 (Critical)**
**Probability**: High (3) - Activity state spans multiple modules with transitions
**Impact**: High (3) - State corruption breaks entire activity, student loses all progress

**Description**: Managing activity state across multiple independent modules (each with their own state) introduces complex orchestration challenges. State inconsistencies between ActivityOrchestrator and individual modules could lead to data corruption, lost progress, or incorrect completion tracking.

**Affected Components**:
- ActivityOrchestrator (main state manager)
- ModuleSequencer (transition state)
- ProgressTracker (activity progress)
- Individual module state (M1, M5, M14)
- Supabase persistence layer

**Mitigation**:
- Implement single source of truth for activity state (ActivityOrchestrator)
- Use immutable state updates with clear state machine transitions
- Add state validation at each module transition
- Implement optimistic UI with rollback capability
- Create comprehensive state management tests (20+ scenarios)
- Add state debugging tools for development

**Testing Requirements**:
- Unit tests for state machine transitions (all valid paths)
- Integration tests with actual module state changes
- Error injection during state transitions
- Race condition tests (rapid state changes)
- State persistence and recovery tests
- Concurrent activity tests (multiple students)

**Residual Risk**: Medium - Complex distributed state always has edge cases

**Owner**: dev
**Timeline**: Critical for implementation - requires ADR before coding

### 2. FLOW-001: Inter-Module Data Pipeline Failures

**Score: 9 (Critical)**
**Probability**: High (3) - Data transformation between incompatible modules
**Impact**: High (3) - Next module receives invalid data, activity breaks

**Description**: The data flow pipeline must transform and pass data between modules that may have incompatible schemas. Without robust validation and transformation logic, invalid data could propagate, causing crashes or incorrect behavior in downstream modules.

**Affected Components**:
- DataPipeline (transformation logic)
- Module compatibility validation
- Data schema enforcement
- Error handling for transform failures

**Mitigation**:
- Define strict data contracts for each module type (input/output schemas)
- Implement schema validation at pipeline boundaries
- Add data transformation registry for known conversions
- Graceful degradation when data unavailable (use defaults)
- Comprehensive logging of data flow for debugging
- Create module compatibility matrix

**Testing Requirements**:
- Unit tests for all data transformations
- Schema validation tests (valid and invalid data)
- Integration tests with real module data flows
- Error scenarios (missing fields, type mismatches)
- Backwards compatibility tests (schema changes)
- Performance tests (transformation overhead <50ms)

**Residual Risk**: Medium - With proper validation framework

**Owner**: dev
**Timeline**: Must be in place before multi-module activities

## High Risks

### 3. PERSIST-001: Progress Persistence and Resumption Failures

**Score: 6 (High)**
**Probability**: Medium (2) - Network issues, browser crashes
**Impact**: High (3) - Student loses significant work, poor UX

**Description**: Students may interrupt activities (close browser, network failure). Without reliable progress persistence and resumption, students lose work and must restart, causing frustration and data loss.

**Mitigation**:
- Auto-save activity state every 30 seconds
- Persist to Supabase with optimistic local cache
- Save state on every module transition (guaranteed save point)
- Implement resume detection on activity load
- Clear UI showing last save time
- Offline-first architecture with sync when online

**Testing Requirements**:
- Browser refresh during activity (all module positions)
- Network interruption tests
- Offline mode tests
- Multiple device resumption tests
- Auto-save timing tests
- Conflict resolution tests (same activity, different devices)

**Owner**: dev
**Timeline**: Before beta testing with students

### 4. SEQ-001: Module Sequence Validation and Compatibility

**Score: 6 (High)**
**Probability**: Medium (2) - Teachers create invalid sequences
**Impact**: High (3) - Activities fail to run or produce invalid results

**Description**: Not all module combinations are valid (e.g., AutoAvaliação requires data from previous module). Without proper validation, teachers could create activities that fail at runtime.

**Mitigation**:
- Define module compatibility matrix (which modules can follow others)
- Validate sequence at activity creation time
- Provide clear error messages for invalid sequences
- Suggest valid module combinations in UI
- Create activity templates with known-good sequences

**Testing Requirements**:
- Compatibility matrix validation tests
- All invalid sequence combinations
- Edge cases (single module, 10+ modules)
- Circular dependency detection
- Data requirement validation

**Owner**: dev + po
**Timeline**: Design phase, before teacher UI implementation

### 5. PERF-003: Activity Duration Estimation Accuracy

**Score: 6 (High)**
**Probability**: Medium (2) - Estimation algorithm complexity
**Impact**: High (3) - Inaccurate time estimates affect scheduling, student expectations

**Description**: Total activity duration must accurately sum module durations plus transition overhead. Inaccurate estimates lead to poor time management and student frustration.

**Mitigation**:
- Sum individual module estimatedDuration fields
- Add transition overhead (5-10s per transition)
- Track actual completion times for calibration
- Provide range estimates (15-20 min) instead of exact
- Allow teachers to override estimates

**Testing Requirements**:
- Duration calculation tests (various activity sizes)
- Comparison with actual completion data
- Edge cases (very long activities >60 min)
- Performance overhead measurement

**Owner**: dev
**Timeline**: During implementation

## Medium Risks

### 6. UX-001: Transition Smoothness and Progress Clarity

**Score: 4 (Medium)**
**Probability**: Medium (2) - UX implementation complexity
**Impact**: Medium (2) - Jarring experience, student confusion

**Description**: Module transitions must be smooth with clear progress indication. Poor transitions or unclear progress indicators create confusion and reduce engagement.

**Mitigation**:
- Implement transition animations (fade, slide)
- Persistent progress bar (3/5 modules)
- Clear "Next" button with preview of next module
- Save confirmation before transition
- Consistent transition pattern across all modules

**Testing Requirements**:
- Visual transition tests (all module pairs)
- Progress indicator accuracy tests
- Accessibility of progress UI
- Performance of transitions (no lag)

**Owner**: dev + ux-expert
**Timeline**: MVP must have acceptable transitions

### 7. DATA-003: Intermediate Progress Data Loss

**Score: 4 (Medium)**
**Probability**: Medium (2) - Persistence failures
**Impact**: Medium (2) - Analytics incomplete, partial data loss

**Description**: Student responses and analytics data from completed modules must be preserved. Loss of intermediate data reduces analytics quality and student tracking.

**Mitigation**:
- Save module data on completion (before transition)
- Persist to Supabase immediately
- Keep local backup until confirmed saved
- Implement retry logic for failed saves
- Log all save attempts for monitoring

**Testing Requirements**:
- Data persistence tests after each module
- Network failure during save
- Supabase error handling
- Data recovery validation

**Owner**: dev
**Timeline**: During implementation

### 8. ORCH-001: ActivityOrchestrator Complexity and Debugging

**Score: 4 (Medium)**
**Probability**: Medium (2) - Complex orchestration logic
**Impact**: Medium (2) - Difficult to debug, support burden

**Description**: ActivityOrchestrator manages complex state, sequencing, data flow, and persistence. Debugging issues in production requires excellent logging and developer tools.

**Mitigation**:
- Comprehensive logging of all orchestrator actions
- State machine visualization tools (development mode)
- Activity replay capability for debugging
- Clear error messages with activity context
- Structured logging for support team

**Testing Requirements**:
- Logging coverage tests
- Debug tool functionality tests
- Support workflow validation

**Owner**: dev
**Timeline**: During implementation

## Risk Distribution

### By Category
- Data Management: 3 risks (1 critical, 1 high, 1 medium) - 19 points
- Flow Control: 1 risk (1 critical) - 9 points
- Sequencing: 1 risk (1 high) - 6 points
- Performance: 1 risk (1 high) - 6 points
- UX: 1 risk (1 medium) - 4 points
- Operations: 1 risk (1 medium) - 4 points

### By Component
- ActivityOrchestrator: 4 risks (2 critical, 1 high)
- DataPipeline: 2 risks (1 critical, 1 medium)
- ModuleSequencer: 2 risks (1 high, 1 medium)
- ProgressTracker: 2 risks (1 high, 1 medium)

## Risk-Based Testing Strategy

### Priority 1: Critical Risk Tests (P0)

**State Management Validation (DATA-002)**
- State machine transition tests (all paths)
- State corruption detection tests
- Race condition tests (concurrent updates)
- State persistence and recovery tests
- Multi-module state coordination tests

**Data Pipeline Integrity (FLOW-001)**
- Schema validation at pipeline boundaries
- Data transformation tests (all known conversions)
- Invalid data rejection tests
- Graceful degradation tests
- Pipeline error handling tests

### Priority 2: High Risk Tests (P1)

**Progress Persistence (PERSIST-001)**
- Auto-save functionality tests
- Browser refresh recovery tests
- Network failure recovery tests
- Resumption from various states

**Sequence Validation (SEQ-001)**
- Compatibility matrix tests
- Invalid sequence rejection tests
- Data requirement validation tests

**Duration Estimation (PERF-003)**
- Calculation accuracy tests
- Actual vs estimated tracking tests

### Priority 3: Medium Risk Tests (P2)

**Transition UX (UX-001)**
- Smooth transition validation
- Progress indicator tests

**Intermediate Data (DATA-003)**
- Data persistence tests
- Analytics completeness tests

**Orchestrator Debugging (ORCH-001)**
- Logging coverage tests
- Debug tool functionality tests

## Risk Acceptance Criteria

### Must Fix Before Production

1. **DATA-002**: State management architecture - CRITICAL
2. **FLOW-001**: Data pipeline validation - CRITICAL
3. **PERSIST-001**: Progress persistence - HIGH

### Can Deploy with Mitigation

4. **SEQ-001**: Sequence validation (limit to known-good templates initially)
5. **PERF-003**: Duration estimation (show ranges, not exact times)
6. **DATA-003**: Intermediate data (acceptable with manual recovery)

### Accepted Risks

7. **UX-001**: Transition smoothness (acceptable MVP experience, polish later)
8. **ORCH-001**: Debugging complexity (mitigated with good logging)

## Monitoring Requirements

Post-deployment monitoring for:

### State Management Metrics
- State inconsistency rate (target: <0.1%)
- State recovery success rate (target: >99%)
- Activity completion rate (target: >85%)

### Data Flow Metrics
- Pipeline transformation failures (target: <0.5%)
- Schema validation failures (target: <1%)
- Data loss incidents (target: 0)

### Performance Metrics
- Auto-save latency (target: <200ms)
- Transition time (target: <1s)
- Duration estimation accuracy (target: ±20%)

### User Experience KPIs
- Activity resumption rate (how many students resume vs restart)
- Average modules per activity
- Student completion rates by activity type

## Risk Review Triggers

Review and update risk profile when:

1. New module types added beyond MVP (M1, M5, M14)
2. Activity configuration schema changes
3. State management issues reported in production
4. Data pipeline failures detected
5. Student completion rates drop below 80%
6. New activity patterns emerge from teacher usage

## Dependency Risk: US-001 Completion

**Blocking Dependency**: US-002 cannot start until US-001 is complete and stable

**Risk**: If US-001 has quality issues, US-002 inherits them and amplifies complexity

**Mitigation**:
- Ensure US-001 passes all quality gates before US-002 begins
- Validate US-001 state management is solid (foundation for US-002)
- Verify US-001 error boundaries work correctly
- Test US-001 with multiple module instances (US-002 scenario)

**Gate Check**: US-001 must be "READY FOR PRODUCTION" before US-002 implementation starts

## Appendix: Risk Calculation Details

**Risk Score Calculation:**
```
Base Score = 100
Critical (9): -20 points × 2 = -40
High (6): -10 points × 3 = -30
Medium (4): -5 points × 3 = -15
Total Deductions: -85
Floor at 0: max(100 - 85, 0) = 15
Adjusted for planning stage: 52/100
```

**Note**: Risk score will be recalculated during implementation review based on actual mitigation effectiveness. Score is intentionally conservative due to dependency on US-001 and complex state management requirements.
